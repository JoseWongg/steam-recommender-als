{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a9d04c4-b34a-4003-8243-3036b308db44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current tracking URI: databricks\nMLflow version:2.1.1\nDatabricks Runtime Version: 12.2.x-cpu-ml-scala2.12\n"
     ]
    }
   ],
   "source": [
    "# Dependencies and configuration\n",
    "# Requires ML runtime\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, trim, lower, when, floor, explode, collect_set, size, udf, sum as spark_sum, round as spark_round, max as spark_max, format_number\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.types import IntegerType, StructType, StructField, DoubleType, StringType, FloatType\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Install and set up MLflow library\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from mlflow.tracking import MlflowClient\n",
    "# Confirm the destination for experiment runs\n",
    "print(\"Current tracking URI:\", mlflow.get_tracking_uri())\n",
    "# Confirm MLflow version\n",
    "print(\"MLflow version:\" + mlflow.__version__)\n",
    "# Import Python's logging module\n",
    "import logging\n",
    "# Set MLflow's logging level to ERROR (To only display ERROR-level and critical log messages)\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.ERROR)\n",
    "# Set the path for the MLflow experiment\n",
    "experiment_path = \"/Users/j.l.wong@edu.salford.ac.uk/game_recommender_system_v3.0\"\n",
    "# Set the MLflow experiment\n",
    "mlflow.set_experiment(experiment_path)\n",
    "# Print runtime version\n",
    "print(\"Databricks Runtime Version:\", spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3ff4279-76b3-46e7-9402-43dfea7e3ca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Introduction##\n",
    "\n",
    "This Databricks notebook addresses the task of building a collaborative filtering recommender system using a dataset extracted from Steam, a video game platform. The dataset contains user/game interaction data based on two types of implicit feedback, game purchases and hours played, and each row captures a unique interaction between a member and a game, including the member’s ID, the game’s name, the type of behaviour (\"play\" or \"purchase\"), and a column indicating either the number of hours played for rows with \"play\" behaviour or a value of 1 for rows with \"purchase\" behaviour. Because games must be purchased before being played, some member/game pairs appear twice in the dataset, each time with different behaviours. The dataset does not include game IDs.\n",
    "\n",
    "The goal is to process this data and use it to train ALS models in Spark MLlib, evaluate their performance and explore the recommendations they generate. Additionally, it is expected that the solution implements an MLflow experiment involving multiple runs for instance using different hyperparameter combinations.\n",
    "\n",
    "The proposed solution implements two modelling approaches, one using only playtime and another combining playtime with a weighted purchase flag as implicit feedback. Several combinations of vector dimensions and regularisation strengths are tested as part of the hyperparameter tuning stage, beginning from baseline models. All the models are evaluated using Root Mean Squared Error (RMSE), Precision@10 and Recall@10 metrics. To demonstrate the use of the trained models, the best Recall and Precision performing models are then used to generate ten personalised recommendations for each member and another ten for a single member. All implementation steps, including the transformation of member and game identifiers into numeric indices, are carried out entirely within Spark and experiment tracking with multiple runs is handled programmatically with MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49489bf3-7f4d-46b7-a40d-9551c60571f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c84096a3-9fa6-411b-8f7d-d9261a5320cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "922fac13-ca94-4e85-9ff5-083989ffc890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Data Loading and Initial Exploration##\n",
    "\n",
    "We started by verifying the dataset has been correctly uploaded to the Databricks FileStore using dbutils.fs.ls().  Then,  we carried out an initial exploration to understand the structure and contents of the dataset, for which we loaded the raw data into a Resilient Distributed Dataset (RDD) using sparkContext.textFile(),  which treats each line of the CSV file as a raw string and displayed a sample with the take() function.\n",
    "\n",
    "After confirming the structure of the dataset (comma-separated values consistent with member ID, game name, behaviour type, and a numerical value representing purchase or playtime), we loaded the dataset into a Spark DataFrame using spark.read.csv(). This keeps all values in string format for better control during early-stage preprocessing.\n",
    "\n",
    "We then performed several basic exploratory checks to understand the dataset’s composition and identify potential data quality issues, including counting total rows, distinct members, and distinct games. Similarly, we checked the unique behaviours recorded (purchase vs play), since these will influence how we treat user feedback, and inspected the presence of nulls in each column to ensure there are no missing values that could affect transformations or model training.\n",
    "\n",
    "Additionally, we used a regular expression to confirm that the hours played column contains valid numeric values in string form (integers or floats), which is what is expected to be used for raitings in the ALS algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17f10253-cd2f-44eb-8291-5f1901e34e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: [FileInfo(path='dbfs:/FileStore/tables/BDTT_Assignment_1_Enron/', name='BDTT_Assignment_1_Enron/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/BDTT_Assignment_1_Enron-1.zip', name='BDTT_Assignment_1_Enron-1.zip', size=375294957, modificationTime=1742269195000),\n FileInfo(path='dbfs:/FileStore/tables/BDTT_Assignment_1_Enron.zip', name='BDTT_Assignment_1_Enron.zip', size=375294957, modificationTime=1739618921000),\n FileInfo(path='dbfs:/FileStore/tables/Clinicaltrial_16012025.csv', name='Clinicaltrial_16012025.csv', size=205522181, modificationTime=1742570281000),\n FileInfo(path='dbfs:/FileStore/tables/Occupancy_Detection_Data.csv', name='Occupancy_Detection_Data.csv', size=50968, modificationTime=1740575789000),\n FileInfo(path='dbfs:/FileStore/tables/account-models/', name='account-models/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/accounts/', name='accounts/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/accounts.zip', name='accounts.zip', size=5297592, modificationTime=1738160594000),\n FileInfo(path='dbfs:/FileStore/tables/activations/', name='activations/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/activations.zip', name='activations.zip', size=8411369, modificationTime=1738156926000),\n FileInfo(path='dbfs:/FileStore/tables/enron_data/', name='enron_data/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/flood-1.zip', name='flood-1.zip', size=52053, modificationTime=1739374138000),\n FileInfo(path='dbfs:/FileStore/tables/flood.csv', name='flood.csv', size=128984, modificationTime=1739374218000),\n FileInfo(path='dbfs:/FileStore/tables/flood.zip', name='flood.zip', size=52053, modificationTime=1739373616000),\n FileInfo(path='dbfs:/FileStore/tables/logs/', name='logs/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/logs.zip', name='logs.zip', size=18168065, modificationTime=1738160609000),\n FileInfo(path='dbfs:/FileStore/tables/movies.csv', name='movies.csv', size=494431, modificationTime=1741547088000),\n FileInfo(path='dbfs:/FileStore/tables/myratings.csv', name='myratings.csv', size=10683, modificationTime=1741547094000),\n FileInfo(path='dbfs:/FileStore/tables/ratings.csv', name='ratings.csv', size=2483723, modificationTime=1741547105000),\n FileInfo(path='dbfs:/FileStore/tables/steam_200k.csv', name='steam_200k.csv', size=8059447, modificationTime=1743164929000),\n FileInfo(path='dbfs:/FileStore/tables/stopwords-1.txt', name='stopwords-1.txt', size=622, modificationTime=1742269046000),\n FileInfo(path='dbfs:/FileStore/tables/stopwords.txt', name='stopwords.txt', size=622, modificationTime=1741131049000),\n FileInfo(path='dbfs:/FileStore/tables/test-1.json', name='test-1.json', size=17958, modificationTime=1737552036000),\n FileInfo(path='dbfs:/FileStore/tables/test.json', name='test.json', size=17958, modificationTime=1737551852000),\n FileInfo(path='dbfs:/FileStore/tables/webpage/', name='webpage/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/webpage.zip', name='webpage.zip', size=1582, modificationTime=1738761361000),\n FileInfo(path='dbfs:/FileStore/tables/webpage_files_all/', name='webpage_files_all/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/webpage_files_jpg/', name='webpage_files_jpg/', size=0, modificationTime=0)]"
     ]
    }
   ],
   "source": [
    "%python\n",
    "# check that the file has been uploaded.\n",
    "dbutils.fs.ls(\"/FileStore/tables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93381740-93ef-4a0d-87d9-e20eb5424633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load raw data into and RDD\n",
    "steamRDD = sc.textFile(\"dbfs:/FileStore/tables/steam_200k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e13aef7d-5620-4e44-99a4-ac8aadf070f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: ['151603712,The Elder Scrolls V Skyrim,purchase,1',\n '151603712,The Elder Scrolls V Skyrim,play,273',\n '151603712,Fallout 4,purchase,1',\n '151603712,Fallout 4,play,87',\n '151603712,Spore,purchase,1',\n '151603712,Spore,play,14.9',\n '151603712,Fallout New Vegas,purchase,1',\n '151603712,Fallout New Vegas,play,12.1',\n '151603712,Left 4 Dead 2,purchase,1',\n '151603712,Left 4 Dead 2,play,8.9']"
     ]
    }
   ],
   "source": [
    "# Display a sample of the data to inspect\n",
    "steamRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00b0030f-2f51-4000-afe6-0e54ed34f0ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loads the data as strings into a DataFrame\n",
    "steamDF_raw = spark.read.csv(\"dbfs:/FileStore/tables/steam_200k.csv\", header=False, inferSchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "298b8087-9ff0-4197-adad-ca2e3cacb67c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------+--------+----+\n|_c0      |_c1                       |_c2     |_c3 |\n+---------+--------------------------+--------+----+\n|151603712|The Elder Scrolls V Skyrim|purchase|1   |\n|151603712|The Elder Scrolls V Skyrim|play    |273 |\n|151603712|Fallout 4                 |purchase|1   |\n|151603712|Fallout 4                 |play    |87  |\n|151603712|Spore                     |purchase|1   |\n|151603712|Spore                     |play    |14.9|\n|151603712|Fallout New Vegas         |purchase|1   |\n|151603712|Fallout New Vegas         |play    |12.1|\n|151603712|Left 4 Dead 2             |purchase|1   |\n|151603712|Left 4 Dead 2             |play    |8.9 |\n+---------+--------------------------+--------+----+\n\n"
     ]
    }
   ],
   "source": [
    "# Display a sample of the DataFrame\n",
    "steamDF_raw.limit(10).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "329d2809-5329-4fe8-931a-57c74914aa86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count:200000\n"
     ]
    }
   ],
   "source": [
    "# Count rows\n",
    "row_count = steamDF_raw.count()\n",
    "print(f\"Row count:{row_count}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7e9a1d5-b2dc-4fba-9893-4bf16c5f90d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Members count: 12393\n"
     ]
    }
   ],
   "source": [
    "# Count of members \n",
    "members_count = steamDF_raw.select((col(\"_c0\"))).distinct().count()\n",
    "print(f\"Members count: {members_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f79f7493-5ed6-4800-b15c-8e17e39c43d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Games count: 5155\n"
     ]
    }
   ],
   "source": [
    "# Count of games\n",
    "games_count = steamDF_raw.select((col(\"_c1\"))).distinct().count()\n",
    "print(f\"Games count: {games_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff2175bd-7a20-4407-956b-2bdc5a06d009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|_c2     |\n+--------+\n|purchase|\n|play    |\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Show all unique behaviours\n",
    "steamDF_raw.select((col(\"_c2\"))).alias(\"behaviour\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b9b8905-bf50-4905-85f7-999a005d52ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of purchase records: 129511\n"
     ]
    }
   ],
   "source": [
    "# Count rows where _c2 == \"purchase\"\n",
    "purchase_count = steamDF_raw.filter(\"_c2 = 'purchase'\").count()\n",
    "print(f\"Number of purchase records: {purchase_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad81e254-67d9-4bc5-b5bc-3c7893a79a14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of play records: 70489\n"
     ]
    }
   ],
   "source": [
    "# Count rows where _c2 == \"play\"\n",
    "purchase_count = steamDF_raw.filter(\"_c2 = 'play'\").count()\n",
    "print(f\"Number of play records: {purchase_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab68dec1-dc7d-4765-81f9-635b5907c2bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with null memberID: 0\nNumber of rows with null game Name: 0\nNumber of rows with null behaviour: 0\nNumber of rows with null hours Played: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "\n",
    "# Dictionary mapping column names to labels for output \n",
    "columns_to_check = {\n",
    "    \"_c0\": \"memberID\",\n",
    "    \"_c1\": \"game Name\",\n",
    "    \"_c2\": \"behaviour\",\n",
    "    \"_c3\": \"hours Played\"\n",
    "}\n",
    "\n",
    "\n",
    "# Iterate through each column to check for null values\n",
    "for col_name, label in columns_to_check.items():\n",
    "    # Count rows where this specific column contains null values\n",
    "    null_count = steamDF_raw.filter(col(col_name).isNull()).count()\n",
    "    # Print the results\n",
    "    print(f\"Number of rows with null {label}: {null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25fa224e-cb75-4010-a813-aa8c57d58c8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with numeric value: 200000\nNumber of rows with dirty value (non-numeric, non-null): 0\n"
     ]
    }
   ],
   "source": [
    "# Check hours played column contains Strings consistent with float  \n",
    "\n",
    "# Defines a regular expresion matching one or more digits followed by an optional group consisting of a dot and a sequence of one or more digits \n",
    "numeric_regex = \"^[0-9]+(\\\\.[0-9]+)?$\"\n",
    "\n",
    "# Count of numeric values (including decimals)\n",
    "numeric_value = steamDF_raw.filter(col(\"_c3\").rlike(numeric_regex)).count()\n",
    "# Print the results\n",
    "print(f\"Number of rows with numeric value: {numeric_value}\")\n",
    "\n",
    "# Count other values not null or not matched by the regex\n",
    "dirty_value = steamDF_raw.filter(~col(\"_c3\").rlike(numeric_regex) & col(\"_c3\").isNotNull()).count()\n",
    "# Print the results\n",
    "print(f\"Number of rows with dirty value (non-numeric, non-null): {dirty_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f29d1fdd-3548-445c-ac22-9571b479d0f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The outputs obtained in the Data Loading and Initial Exploration stage confirmed that the dataset was successfully loaded and contains 200,000 member/game interaction records, involving 12,393 unique members and 5,155 unique games. No missing values were found in any column, \n",
    "\n",
    "Although the dataset did not have headers, the four columns were consistent with the description of the task's expected values. One of the columns contained the String distinct values \"purchase\" and \"play\", consistent with the behaviours to be used for implicit feedback. There was a column consistent with a numeric value to be used as a user ID. Another column  showed valid numeric Strings that can be cast to a numeric data type for ratings. Except for the lack of an item ID for games, expected by the ALS algorithm, and the presence of instances where the same member had two rows due to having purchased and played a game, the data was found to be suitable to allow for the development of the collaborative filtering solution with implicit feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "221e0c63-a0a6-47ff-8481-e54071d850c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efc9fe1b-2b60-4168-ba49-6c0165a850f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3e38eb9-87f2-4a83-8025-18332aa60862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Data Pre-Processing##\n",
    "We began this stage by giving the columns meaningful names: memberID, gameName, behaviour, and hoursPlayed for clarity, using the `withColumnRenamed()` method of the DataFrame class. \n",
    "\n",
    "Next, we casted the hoursPlayed column to float as this column holds the values for hours played or one purchase which needed to be of numeric data type to be used as rating in the ALS algorithm.\n",
    "\n",
    "Additionally, we ensured that whenever the behaviour column indicated a \"purchase\", the corresponding hoursPlayed value was overwritten as 1.0 in `steamDF = steamDF.withColumn(\"hoursPlayed\", when(lower(trim(col(\"behaviour\"))) == \"purchase\", 1.0).otherwise(col(\"hoursPlayed\")))`, to  correct for any inconsistent values that may have been mistakenly recorded to indicate purchase in the hoursPlayed column.\n",
    "\n",
    "Given that the Spark interface to access the ALS algorithm also requires both user (member) and item (game) integer IDs, we addressed the fact that, in the dataset, memberID was of String data type and there was no numeric identifier for games by transforming memberID and gameName both using PySpark's StringIndexer to produce the memberIndex and gameIndex columns, mapping each unique String value to a numeric index. These columns were then casted to Integer type to satisfy the input requirements of Spark’s ALS implementation.\n",
    "\n",
    "Considering that, in the dataset, the same member can have two entries for the same game, one with the behaviour value \"purchase\", and one with \"play\", we  pivoted the data to unify the information so that instead of keeping separate rows for each behaviour, we generated a single row per member, with one column, `played_hours`, indicating hours played and another, `purchase_flag`, indicating whether the game was purchased. Missing values in either column were filled with zero. \n",
    "\n",
    "Subsequently,  we created two DataFrames that would allow us to model two variants of implicit feedback. The first DataFrame, steamDF_playOnly,  intended for training ALS on playtime alone, was prepared by selecting only the `played_hours` column as the rating and filtering out all zero-hour rows. The second DataFrame, steamDF_playAndPurchase, was constructed by computing a new rating as the sum of `played_hours` and `purchased_flag` multiplied by a weight set to 5.0, indicating the relative importance of the purchase implicit feedback for the model, a hyperparameter that can be adjusted based on domain knowledge or for model tuning. This way, we combined both types of relationships between a member and a game into a single implicit feedback rating. In both DataFrames, we kept only the relevant columns (memberIndex, gameIndex, and rating) as expected by ALS.\n",
    "\n",
    "Given that ALS with implicit feedback treats higher implicit feedback values as stronger signals of user preference, the model could have been at risk of overfitting due to the presence of outliers with extremely high rating values — for instance, prolonged game sessions accumulating to thousands of hours. To understand the distribution of ratings in both DataFrames, we used .describe() to calculate summary statistics such as count, mean, standard deviation, minimum, and maximum. We then visualised the distributions by binning the ratings into 100 intervals using `(floor(col(\"rating\") / 100) * 100)` and registering the resulting DataFrames as temporary views. These were then queried via `SELECT * FROM SQL` statements and visualised as bar charts using the Databricks built-in visualisation functionality.\n",
    "\n",
    "The resulting charts confirmed a heavy skew in both DataFrames, with most feedback in the 0-100 bin, and a long tail of high values. To mitigate this skew and reduce the effect of extreme outliers, we computed the 95th percentile of the rating values in both datasets using `approxQuantile(\"rating\", [0.95], 0.01)[0]`. For Play Only, the threshold was 152.0, and for Play and Purchase (considering only play values), it was 72.0. Ratings above these thresholds were capped to their respective 95th percentile values. This helped stabilise the data distributions while preserving relative differences in implicit feedback strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b02fe7d1-8b5f-4545-8e0e-ee7b0e6c2adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rename columns for clarity\n",
    "steamDF = steamDF_raw.withColumnRenamed(\"_c0\", \"memberID\") \\\n",
    "                     .withColumnRenamed(\"_c1\", \"gameName\") \\\n",
    "                     .withColumnRenamed(\"_c2\", \"behaviour\") \\\n",
    "                     .withColumnRenamed(\"_c3\", \"hoursPlayed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f9a0d6a-c1fe-43e0-83cf-2eddc9a8fe10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cast \"hoursPlayed\" column from String to float\n",
    "steamDF = steamDF.withColumn(\"hoursPlayed\", col(\"hoursPlayed\").cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "586fdec0-4317-4b31-90ca-17aeea1c6c68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Overwrites \"hoursPlayed\" column to ensure that when the behaviour column value is \"purchase\", the \"hoursPlayed\" column value is always 1, correcting inconsistent values\n",
    "steamDF = steamDF.withColumn(\"hoursPlayed\", when(lower(trim(col(\"behaviour\"))) == \"purchase\", 1.0).otherwise(col(\"hoursPlayed\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2824623d-12de-4af1-8208-554b743647bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pivot and combine play/purchase\n",
    "# convert each member/game pair into a single row with both playtime and purchase info.\n",
    "\n",
    "# Pivot the behaviour column to get separate columns for 'play' and 'purchase'\n",
    "pivotedDF = steamDF.groupBy(\"memberID\", \"gameName\").pivot(\"behaviour\").agg(spark_max(\"hoursPlayed\"))\n",
    "\n",
    "# Rename the new columns to make them clearer\n",
    "pivotedDF = pivotedDF.withColumnRenamed(\"play\", \"played_hours\").withColumnRenamed(\"purchase\", \"purchased_flag\")\n",
    "\n",
    "# Replace nulls resulting from the pivoting with 0\n",
    "pivotedDF = pivotedDF.fillna(0, subset=[\"played_hours\", \"purchased_flag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8ab377-aa41-483a-97d3-a62101772371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- memberID: string (nullable = true)\n |-- gameName: string (nullable = true)\n |-- played_hours: double (nullable = false)\n |-- purchased_flag: double (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "pivotedDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6c20a14-c112-4411-b391-e6632a6ab394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>memberID</th><th>gameName</th><th>played_hours</th><th>purchased_flag</th></tr></thead><tbody><tr><td>60296891</td><td>Batman Arkham Asylum GOTY Edition</td><td>0.0</td><td>1.0</td></tr><tr><td>86055705</td><td>Destination Sol</td><td>0.0</td><td>1.0</td></tr><tr><td>30246419</td><td>Thief Deadly Shadows</td><td>0.0</td><td>1.0</td></tr><tr><td>64479113</td><td>Team Fortress Classic</td><td>0.0</td><td>1.0</td></tr><tr><td>80164199</td><td>FTL Faster Than Light</td><td>11.8</td><td>1.0</td></tr><tr><td>158944454</td><td>Warface</td><td>1.4</td><td>1.0</td></tr><tr><td>75328197</td><td>Call of Duty Black Ops - Multiplayer</td><td>5.8</td><td>1.0</td></tr><tr><td>105782521</td><td>Terraria</td><td>0.2</td><td>1.0</td></tr><tr><td>105384518</td><td>Lunar Flight</td><td>5.5</td><td>1.0</td></tr><tr><td>33706322</td><td>Saints Row The Third</td><td>2.0</td><td>1.0</td></tr><tr><td>27117407</td><td>Half-Life 2</td><td>0.0</td><td>1.0</td></tr><tr><td>72207742</td><td>Age of Empires II HD Edition</td><td>93.0</td><td>1.0</td></tr><tr><td>49462664</td><td>F.E.A.R. 2 Project Origin</td><td>0.0</td><td>1.0</td></tr><tr><td>28648610</td><td>Ricochet</td><td>0.0</td><td>1.0</td></tr><tr><td>80189887</td><td>Natural Selection 2</td><td>25.0</td><td>1.0</td></tr><tr><td>33706322</td><td>Counter-Strike</td><td>0.0</td><td>1.0</td></tr><tr><td>62705916</td><td>Battlestations Pacific</td><td>0.0</td><td>1.0</td></tr><tr><td>67713900</td><td>Pride of Nations</td><td>5.3</td><td>1.0</td></tr><tr><td>72842694</td><td>Strider</td><td>1.2</td><td>1.0</td></tr><tr><td>39622853</td><td>Metro 2033</td><td>0.7</td><td>1.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "60296891",
         "Batman Arkham Asylum GOTY Edition",
         0.0,
         1.0
        ],
        [
         "86055705",
         "Destination Sol",
         0.0,
         1.0
        ],
        [
         "30246419",
         "Thief Deadly Shadows",
         0.0,
         1.0
        ],
        [
         "64479113",
         "Team Fortress Classic",
         0.0,
         1.0
        ],
        [
         "80164199",
         "FTL Faster Than Light",
         11.8,
         1.0
        ],
        [
         "158944454",
         "Warface",
         1.4,
         1.0
        ],
        [
         "75328197",
         "Call of Duty Black Ops - Multiplayer",
         5.8,
         1.0
        ],
        [
         "105782521",
         "Terraria",
         0.2,
         1.0
        ],
        [
         "105384518",
         "Lunar Flight",
         5.5,
         1.0
        ],
        [
         "33706322",
         "Saints Row The Third",
         2.0,
         1.0
        ],
        [
         "27117407",
         "Half-Life 2",
         0.0,
         1.0
        ],
        [
         "72207742",
         "Age of Empires II HD Edition",
         93.0,
         1.0
        ],
        [
         "49462664",
         "F.E.A.R. 2 Project Origin",
         0.0,
         1.0
        ],
        [
         "28648610",
         "Ricochet",
         0.0,
         1.0
        ],
        [
         "80189887",
         "Natural Selection 2",
         25.0,
         1.0
        ],
        [
         "33706322",
         "Counter-Strike",
         0.0,
         1.0
        ],
        [
         "62705916",
         "Battlestations Pacific",
         0.0,
         1.0
        ],
        [
         "67713900",
         "Pride of Nations",
         5.3,
         1.0
        ],
        [
         "72842694",
         "Strider",
         1.2,
         1.0
        ],
        [
         "39622853",
         "Metro 2033",
         0.7,
         1.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "memberID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gameName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "played_hours",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "purchased_flag",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show sample of the pivoted DataFrame\n",
    "pivotedDF.withColumn(\"played_hours\", spark_round(col(\"played_hours\"), 2)).limit(20).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54956b56-a1c5-4db4-a25b-f46bcb4e9a0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create numeric indexes for members and games using the memberID and gameName Strings\n",
    "# Required for ALS. Spark MLlib needs user and item IDs as integers or floats\n",
    "\n",
    "# Instantiates and object of the Class StringIndexer to map each distinct memberID (string) to a numeric index\n",
    "user_indexer = StringIndexer(\n",
    "    inputCol=\"memberID\",     # The source column with user identifiers (Strings)\n",
    "    outputCol=\"memberIndex\", # The destination column for numeric user IDs (the generated indices)\n",
    "    handleInvalid=\"skip\"     # Skip rows with unexpected/null values\n",
    ")\n",
    "\n",
    "# The user_indexer StringIndexer scans the memberID column to learn the mapping by identifying all unique values and assigning a numeric index to each (for instance memberID '151603712' generates memberIndex 0.0). \n",
    "# Returns a StringIndexModel\n",
    "member_indexer_model = user_indexer.fit(pivotedDF)\n",
    "\n",
    "# Applies the memberIndexerModel with the mapping to the DataFrame. \n",
    "# Returns the new DataFrame with the 'memberIndex' column.\n",
    "pivotedDF =  member_indexer_model.transform(pivotedDF)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instantiates and object of the Class StringIndexer to map each distinct gameName (String) to a numeric index\n",
    "game_indexer = StringIndexer(\n",
    "    inputCol=\"gameName\",          # The source column with game names\n",
    "    outputCol=\"gameIndex\",      # The destination column for numeric game IDs (the generated indices)\n",
    "    handleInvalid=\"skip\"      # Skip rows with unexpected/null values\n",
    ")\n",
    "\n",
    "# The game_indexer StringIndexer scans the gameName column to learn the mapping by identifying all unique values and assigning a numeric index to each (for instance gameName 'The Elder Scrolls V Skyrim' generates gameIndex 0.0). \n",
    "# Returns a StringIndexModel\n",
    "game_indexer_model = game_indexer.fit(pivotedDF)\n",
    "\n",
    "# Applies the game_Indexer_Model with the mapping to the DataFrame. \n",
    "# Returns the new DataFrame with the 'gameIndex' column.\n",
    "pivotedDF = game_indexer_model.transform(pivotedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a3da6e9-3376-4cca-9f01-1b7636ec87b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cast memberIndex and gameIndex to IntegerType for ALS compatibility\n",
    "pivotedDF = pivotedDF.withColumn(\"memberIndex\", col(\"memberIndex\").cast(IntegerType()))\n",
    "pivotedDF = pivotedDF.withColumn(\"gameIndex\", col(\"gameIndex\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45be96ad-3a99-4fb1-88dd-da6c35ddec14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>memberID</th><th>gameName</th><th>played_hours</th><th>purchased_flag</th><th>memberIndex</th><th>gameIndex</th></tr></thead><tbody><tr><td>60296891</td><td>Batman Arkham Asylum GOTY Edition</td><td>0.0</td><td>1.0</td><td>229</td><td>123</td></tr><tr><td>86055705</td><td>Destination Sol</td><td>0.0</td><td>1.0</td><td>50</td><td>1352</td></tr><tr><td>30246419</td><td>Thief Deadly Shadows</td><td>0.0</td><td>1.0</td><td>2</td><td>559</td></tr><tr><td>64479113</td><td>Team Fortress Classic</td><td>0.0</td><td>1.0</td><td>4321</td><td>37</td></tr><tr><td>80164199</td><td>FTL Faster Than Light</td><td>11.8</td><td>1.0</td><td>2170</td><td>150</td></tr><tr><td>158944454</td><td>Warface</td><td>1.4</td><td>1.0</td><td>2046</td><td>48</td></tr><tr><td>75328197</td><td>Call of Duty Black Ops - Multiplayer</td><td>5.8</td><td>1.0</td><td>6545</td><td>70</td></tr><tr><td>105782521</td><td>Terraria</td><td>0.2</td><td>1.0</td><td>413</td><td>24</td></tr><tr><td>105384518</td><td>Lunar Flight</td><td>5.5</td><td>1.0</td><td>303</td><td>1832</td></tr><tr><td>33706322</td><td>Saints Row The Third</td><td>2.0</td><td>1.0</td><td>292</td><td>64</td></tr><tr><td>27117407</td><td>Half-Life 2</td><td>0.0</td><td>1.0</td><td>1731</td><td>16</td></tr><tr><td>72207742</td><td>Age of Empires II HD Edition</td><td>93.0</td><td>1.0</td><td>2000</td><td>83</td></tr><tr><td>49462664</td><td>F.E.A.R. 2 Project Origin</td><td>0.0</td><td>1.0</td><td>70</td><td>213</td></tr><tr><td>28648610</td><td>Ricochet</td><td>0.0</td><td>1.0</td><td>1647</td><td>23</td></tr><tr><td>80189887</td><td>Natural Selection 2</td><td>25.0</td><td>1.0</td><td>183</td><td>191</td></tr><tr><td>33706322</td><td>Counter-Strike</td><td>0.0</td><td>1.0</td><td>292</td><td>7</td></tr><tr><td>62705916</td><td>Battlestations Pacific</td><td>0.0</td><td>1.0</td><td>278</td><td>1202</td></tr><tr><td>67713900</td><td>Pride of Nations</td><td>5.3</td><td>1.0</td><td>1254</td><td>3752</td></tr><tr><td>72842694</td><td>Strider</td><td>1.2</td><td>1.0</td><td>115</td><td>1109</td></tr><tr><td>39622853</td><td>Metro 2033</td><td>0.7</td><td>1.0</td><td>126</td><td>51</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "60296891",
         "Batman Arkham Asylum GOTY Edition",
         0.0,
         1.0,
         229,
         123
        ],
        [
         "86055705",
         "Destination Sol",
         0.0,
         1.0,
         50,
         1352
        ],
        [
         "30246419",
         "Thief Deadly Shadows",
         0.0,
         1.0,
         2,
         559
        ],
        [
         "64479113",
         "Team Fortress Classic",
         0.0,
         1.0,
         4321,
         37
        ],
        [
         "80164199",
         "FTL Faster Than Light",
         11.8,
         1.0,
         2170,
         150
        ],
        [
         "158944454",
         "Warface",
         1.4,
         1.0,
         2046,
         48
        ],
        [
         "75328197",
         "Call of Duty Black Ops - Multiplayer",
         5.8,
         1.0,
         6545,
         70
        ],
        [
         "105782521",
         "Terraria",
         0.2,
         1.0,
         413,
         24
        ],
        [
         "105384518",
         "Lunar Flight",
         5.5,
         1.0,
         303,
         1832
        ],
        [
         "33706322",
         "Saints Row The Third",
         2.0,
         1.0,
         292,
         64
        ],
        [
         "27117407",
         "Half-Life 2",
         0.0,
         1.0,
         1731,
         16
        ],
        [
         "72207742",
         "Age of Empires II HD Edition",
         93.0,
         1.0,
         2000,
         83
        ],
        [
         "49462664",
         "F.E.A.R. 2 Project Origin",
         0.0,
         1.0,
         70,
         213
        ],
        [
         "28648610",
         "Ricochet",
         0.0,
         1.0,
         1647,
         23
        ],
        [
         "80189887",
         "Natural Selection 2",
         25.0,
         1.0,
         183,
         191
        ],
        [
         "33706322",
         "Counter-Strike",
         0.0,
         1.0,
         292,
         7
        ],
        [
         "62705916",
         "Battlestations Pacific",
         0.0,
         1.0,
         278,
         1202
        ],
        [
         "67713900",
         "Pride of Nations",
         5.3,
         1.0,
         1254,
         3752
        ],
        [
         "72842694",
         "Strider",
         1.2,
         1.0,
         115,
         1109
        ],
        [
         "39622853",
         "Metro 2033",
         0.7,
         1.0,
         126,
         51
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "memberID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gameName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "played_hours",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "purchased_flag",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "memberIndex",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "gameIndex",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show sample of the pivoted DataFrame with the gameIndex and memberIndex column\n",
    "pivotedDF.withColumn(\"played_hours\", spark_round(col(\"played_hours\"), 2)).limit(20).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e15e71a-e72b-44c5-9b34-cca52b9d115f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- memberID: string (nullable = true)\n |-- gameName: string (nullable = true)\n |-- played_hours: double (nullable = false)\n |-- purchased_flag: double (nullable = false)\n |-- memberIndex: integer (nullable = true)\n |-- gameIndex: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Show the pre-processed DataFrame schema to confirm memberIndex and gameIndex casting\n",
    "pivotedDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71b5ca17-6ab5-419f-a9d6-bc336c17380a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>memberIndex</th><th>gameIndex</th><th>rating</th></tr></thead><tbody><tr><td>2170</td><td>150</td><td>11.8</td></tr><tr><td>2046</td><td>48</td><td>1.4</td></tr><tr><td>6545</td><td>70</td><td>5.8</td></tr><tr><td>413</td><td>24</td><td>0.2</td></tr><tr><td>303</td><td>1832</td><td>5.5</td></tr><tr><td>292</td><td>64</td><td>2.0</td></tr><tr><td>2000</td><td>83</td><td>93.0</td></tr><tr><td>183</td><td>191</td><td>25.0</td></tr><tr><td>1254</td><td>3752</td><td>5.3</td></tr><tr><td>115</td><td>1109</td><td>1.2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2170,
         150,
         11.8
        ],
        [
         2046,
         48,
         1.4
        ],
        [
         6545,
         70,
         5.8
        ],
        [
         413,
         24,
         0.2
        ],
        [
         303,
         1832,
         5.5
        ],
        [
         292,
         64,
         2.0
        ],
        [
         2000,
         83,
         93.0
        ],
        [
         183,
         191,
         25.0
        ],
        [
         1254,
         3752,
         5.3
        ],
        [
         115,
         1109,
         1.2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "memberIndex",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "gameIndex",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "rating",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepares a DataFrame ready for a model that uses hours played as implicit feedback\n",
    "\n",
    "# Rename the column to be used as rating (played_hours) as \"rating\" for consitency with the ALS paramaters name\n",
    "steamDF_playOnly = pivotedDF.withColumnRenamed(\"played_hours\", \"rating\")\n",
    "\n",
    "\n",
    "# Keep only the relevant columns and where rating (hours played) is not zero \n",
    "# This represents the list of member/game iteration which will be inputed into the ALS algorithm\n",
    "steamDF_playOnly = steamDF_playOnly.select(\"memberIndex\", \"gameIndex\", \"rating\").filter(\"rating != 0\")\n",
    "\n",
    "# Show a preview\n",
    "steamDF_playOnly.withColumn(\"rating\", spark_round(col(\"rating\"), 2)).limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4facee7-4206-484f-bfaa-12ff18d93c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings summary - play only:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>summary</th><th>rating</th></tr></thead><tbody><tr><td>count</td><td>70,477.00</td></tr><tr><td>mean</td><td>48.89</td></tr><tr><td>stddev</td><td>229.35</td></tr><tr><td>min</td><td>0.10</td></tr><tr><td>max</td><td>11,754.00</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "count",
         "70,477.00"
        ],
        [
         "mean",
         "48.89"
        ],
        [
         "stddev",
         "229.35"
        ],
        [
         "min",
         "0.10"
        ],
        [
         "max",
         "11,754.00"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "summary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rating",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Describe min/max/avg values of the play only rating\n",
    "print(\"Ratings summary - play only:\")\n",
    "steamDF_playOnly.describe(\"rating\").withColumn(\"rating\",format_number(col(\"rating\").cast(\"double\"), 2)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7002d8b1-cf9b-4f1f-9108-6ee85482c0be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bins playtime into 100 intervals to create a frequency distribution\n",
    "\n",
    "# Bin the rating values (group into 100-hour buckets)\n",
    "playOnly_binned = steamDF_playOnly.withColumn(\"rating_bin\", (floor(col(\"rating\") / 100) * 100))\n",
    "\n",
    "# Create a new DataFrame with the count of user-game interactions in each 100-hour rating bin representing a frequency distribution of playtime across buckets\n",
    "rating_distribution_playOnly = playOnly_binned.groupBy(\"rating_bin\").count().orderBy(\"rating_bin\")\n",
    "\n",
    "# Register the new DataFrame as a temp view\n",
    "rating_distribution_playOnly.createOrReplaceTempView(\"rating_distribution_playonly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57ecd706-dc2e-442d-93e2-28f8bdd5b602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>rating_bin</th><th>count</th></tr></thead><tbody><tr><td>0</td><td>64696</td></tr><tr><td>100</td><td>2399</td></tr><tr><td>200</td><td>1006</td></tr><tr><td>300</td><td>536</td></tr><tr><td>400</td><td>392</td></tr><tr><td>500</td><td>267</td></tr><tr><td>600</td><td>176</td></tr><tr><td>700</td><td>155</td></tr><tr><td>800</td><td>134</td></tr><tr><td>900</td><td>90</td></tr><tr><td>1000</td><td>96</td></tr><tr><td>1100</td><td>72</td></tr><tr><td>1200</td><td>57</td></tr><tr><td>1300</td><td>47</td></tr><tr><td>1400</td><td>46</td></tr><tr><td>1500</td><td>31</td></tr><tr><td>1600</td><td>25</td></tr><tr><td>1700</td><td>29</td></tr><tr><td>1800</td><td>26</td></tr><tr><td>1900</td><td>21</td></tr><tr><td>2000</td><td>15</td></tr><tr><td>2100</td><td>12</td></tr><tr><td>2200</td><td>13</td></tr><tr><td>2300</td><td>9</td></tr><tr><td>2400</td><td>12</td></tr><tr><td>2500</td><td>11</td></tr><tr><td>2600</td><td>13</td></tr><tr><td>2700</td><td>6</td></tr><tr><td>2800</td><td>7</td></tr><tr><td>2900</td><td>5</td></tr><tr><td>3000</td><td>5</td></tr><tr><td>3100</td><td>3</td></tr><tr><td>3200</td><td>3</td></tr><tr><td>3300</td><td>5</td></tr><tr><td>3400</td><td>3</td></tr><tr><td>3500</td><td>3</td></tr><tr><td>3600</td><td>5</td></tr><tr><td>3700</td><td>5</td></tr><tr><td>3800</td><td>2</td></tr><tr><td>3900</td><td>1</td></tr><tr><td>4000</td><td>3</td></tr><tr><td>4100</td><td>1</td></tr><tr><td>4200</td><td>2</td></tr><tr><td>4300</td><td>2</td></tr><tr><td>4400</td><td>2</td></tr><tr><td>4600</td><td>5</td></tr><tr><td>4700</td><td>3</td></tr><tr><td>4800</td><td>3</td></tr><tr><td>5000</td><td>1</td></tr><tr><td>5100</td><td>1</td></tr><tr><td>5200</td><td>1</td></tr><tr><td>5400</td><td>1</td></tr><tr><td>5600</td><td>1</td></tr><tr><td>5700</td><td>1</td></tr><tr><td>5800</td><td>1</td></tr><tr><td>5900</td><td>2</td></tr><tr><td>6000</td><td>2</td></tr><tr><td>6700</td><td>1</td></tr><tr><td>6900</td><td>1</td></tr><tr><td>7700</td><td>1</td></tr><tr><td>9600</td><td>1</td></tr><tr><td>10400</td><td>1</td></tr><tr><td>11700</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         64696
        ],
        [
         100,
         2399
        ],
        [
         200,
         1006
        ],
        [
         300,
         536
        ],
        [
         400,
         392
        ],
        [
         500,
         267
        ],
        [
         600,
         176
        ],
        [
         700,
         155
        ],
        [
         800,
         134
        ],
        [
         900,
         90
        ],
        [
         1000,
         96
        ],
        [
         1100,
         72
        ],
        [
         1200,
         57
        ],
        [
         1300,
         47
        ],
        [
         1400,
         46
        ],
        [
         1500,
         31
        ],
        [
         1600,
         25
        ],
        [
         1700,
         29
        ],
        [
         1800,
         26
        ],
        [
         1900,
         21
        ],
        [
         2000,
         15
        ],
        [
         2100,
         12
        ],
        [
         2200,
         13
        ],
        [
         2300,
         9
        ],
        [
         2400,
         12
        ],
        [
         2500,
         11
        ],
        [
         2600,
         13
        ],
        [
         2700,
         6
        ],
        [
         2800,
         7
        ],
        [
         2900,
         5
        ],
        [
         3000,
         5
        ],
        [
         3100,
         3
        ],
        [
         3200,
         3
        ],
        [
         3300,
         5
        ],
        [
         3400,
         3
        ],
        [
         3500,
         3
        ],
        [
         3600,
         5
        ],
        [
         3700,
         5
        ],
        [
         3800,
         2
        ],
        [
         3900,
         1
        ],
        [
         4000,
         3
        ],
        [
         4100,
         1
        ],
        [
         4200,
         2
        ],
        [
         4300,
         2
        ],
        [
         4400,
         2
        ],
        [
         4600,
         5
        ],
        [
         4700,
         3
        ],
        [
         4800,
         3
        ],
        [
         5000,
         1
        ],
        [
         5100,
         1
        ],
        [
         5200,
         1
        ],
        [
         5400,
         1
        ],
        [
         5600,
         1
        ],
        [
         5700,
         1
        ],
        [
         5800,
         1
        ],
        [
         5900,
         2
        ],
        [
         6000,
         2
        ],
        [
         6700,
         1
        ],
        [
         6900,
         1
        ],
        [
         7700,
         1
        ],
        [
         9600,
         1
        ],
        [
         10400,
         1
        ],
        [
         11700,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "rating_bin",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql WITH q AS (SELECT * FROM rating_distribution_playonly) SELECT `rating_bin`,SUM(`count`) `column_94f965f5304` FROM q GROUP BY `rating_bin`",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "rating_bin",
             "id": "column_94f965f5303"
            },
            "y": [
             {
              "column": "count",
              "id": "column_94f965f5304",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_94f965f5304": {
             "name": "count",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "implicitDf": true,
        "rowLimit": 10000
       },
       "nuid": "f8f05ebc-94ea-4ce5-bea0-e01f916b31d6",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 2.7496509552001953,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "rating_bin",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "rating_bin",
           "type": "column"
          },
          {
           "alias": "column_94f965f5304",
           "args": [
            {
             "column": "count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "--Query the binned playtime distribution from the temporary view to show how many user-game interactions fall into each 100-hour rating bucket.\n",
    "-- The result table is used to produce a bar chart for visualisation\n",
    "SELECT * FROM rating_distribution_playonly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e657acfa-551a-4267-b074-4692bbf51d69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>memberIndex</th><th>gameIndex</th><th>rating</th></tr></thead><tbody><tr><td>229</td><td>123</td><td>5.0</td></tr><tr><td>50</td><td>1352</td><td>5.0</td></tr><tr><td>2</td><td>559</td><td>5.0</td></tr><tr><td>4321</td><td>37</td><td>5.0</td></tr><tr><td>2170</td><td>150</td><td>16.8</td></tr><tr><td>2046</td><td>48</td><td>6.4</td></tr><tr><td>6545</td><td>70</td><td>10.8</td></tr><tr><td>413</td><td>24</td><td>5.2</td></tr><tr><td>303</td><td>1832</td><td>10.5</td></tr><tr><td>292</td><td>64</td><td>7.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         229,
         123,
         5.0
        ],
        [
         50,
         1352,
         5.0
        ],
        [
         2,
         559,
         5.0
        ],
        [
         4321,
         37,
         5.0
        ],
        [
         2170,
         150,
         16.8
        ],
        [
         2046,
         48,
         6.4
        ],
        [
         6545,
         70,
         10.8
        ],
        [
         413,
         24,
         5.2
        ],
        [
         303,
         1832,
         10.5
        ],
        [
         292,
         64,
         7.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "memberIndex",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "gameIndex",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "rating",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepares a DataFrame ready for a model that combines \"play\" and \"purchase\" behaviours as feedback\n",
    "\n",
    "# Scale the weight of \"purchase\" in the rating \n",
    "# Different weights for purchase can be assigned based on the domain knowledge and the relative importance of purchase vs hours played or for experimentation and tuning \n",
    "purchase_weight = 5.0\n",
    "\n",
    "# Combines playtime and purchase info into a single rating to serve as unified implicit feedback\n",
    "# This also renames the \"hours_played\" column to \"rating\" for consistency with the ALS parameter name\n",
    "steamDF_playAndPurchase = pivotedDF.withColumn(\"rating\", col(\"played_hours\") + (col(\"purchased_flag\") * purchase_weight))\n",
    "\n",
    "# Select ALS-required columns only\n",
    "steamDF_playAndPurchase = steamDF_playAndPurchase.select(\"memberIndex\", \"gameIndex\", \"rating\")\n",
    "\n",
    "# Show a preview\n",
    "steamDF_playAndPurchase.withColumn(\"rating\", spark_round(col(\"rating\"), 2)).limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2615e29c-c5a8-4d71-a0d9-23c3523ecb0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings summary - purchase + play:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>summary</th><th>rating</th></tr></thead><tbody><tr><td>count</td><td>128,804.00</td></tr><tr><td>mean</td><td>31.75</td></tr><tr><td>stddev</td><td>171.39</td></tr><tr><td>min</td><td>5.00</td></tr><tr><td>max</td><td>11,759.00</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "count",
         "128,804.00"
        ],
        [
         "mean",
         "31.75"
        ],
        [
         "stddev",
         "171.39"
        ],
        [
         "min",
         "5.00"
        ],
        [
         "max",
         "11,759.00"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "summary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rating",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Describe min/max/avg values of the play and purchase rating\n",
    "print(\"Ratings summary - purchase + play:\")\n",
    "steamDF_playAndPurchase.describe(\"rating\").withColumn(\"rating\",format_number(col(\"rating\").cast(\"double\"), 2)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a6a709-ba68-40f4-8a14-90a5d4acf69a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bins rating values into intervals of 100 to create a frequency distribution\n",
    "\n",
    "# Bin the rating values in steamDF_playAndPurchase)\n",
    "playAndPurchase_binned = steamDF_playAndPurchase.withColumn(\"rating_bin\", (floor(col(\"rating\") / 100) * 100))\n",
    "\n",
    "# Create a new DataFrame with the count of user-game interactions in each 100-unit rating bin, representing a frequency distribution across rating ranges\n",
    "rating_distribution_playAndPurchase = playAndPurchase_binned.groupBy(\"rating_bin\").count().orderBy(\"rating_bin\")\n",
    "\n",
    "# Register the new DataFrame as a temp view\n",
    "rating_distribution_playAndPurchase.createOrReplaceTempView(\"rating_distribution_playAndPurchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ab548bd-9b6a-49a2-8842-c6df8637471b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>rating_bin</th><th>count</th></tr></thead><tbody><tr><td>0</td><td>122809</td></tr><tr><td>100</td><td>2548</td></tr><tr><td>200</td><td>1032</td></tr><tr><td>300</td><td>557</td></tr><tr><td>400</td><td>393</td></tr><tr><td>500</td><td>276</td></tr><tr><td>600</td><td>178</td></tr><tr><td>700</td><td>148</td></tr><tr><td>800</td><td>140</td></tr><tr><td>900</td><td>92</td></tr><tr><td>1000</td><td>98</td></tr><tr><td>1100</td><td>71</td></tr><tr><td>1200</td><td>57</td></tr><tr><td>1300</td><td>49</td></tr><tr><td>1400</td><td>42</td></tr><tr><td>1500</td><td>36</td></tr><tr><td>1600</td><td>25</td></tr><tr><td>1700</td><td>29</td></tr><tr><td>1800</td><td>23</td></tr><tr><td>1900</td><td>23</td></tr><tr><td>2000</td><td>17</td></tr><tr><td>2100</td><td>12</td></tr><tr><td>2200</td><td>11</td></tr><tr><td>2300</td><td>11</td></tr><tr><td>2400</td><td>11</td></tr><tr><td>2500</td><td>12</td></tr><tr><td>2600</td><td>13</td></tr><tr><td>2700</td><td>6</td></tr><tr><td>2800</td><td>7</td></tr><tr><td>2900</td><td>5</td></tr><tr><td>3000</td><td>5</td></tr><tr><td>3100</td><td>3</td></tr><tr><td>3200</td><td>3</td></tr><tr><td>3300</td><td>5</td></tr><tr><td>3400</td><td>3</td></tr><tr><td>3500</td><td>3</td></tr><tr><td>3600</td><td>4</td></tr><tr><td>3700</td><td>6</td></tr><tr><td>3800</td><td>1</td></tr><tr><td>3900</td><td>2</td></tr><tr><td>4000</td><td>3</td></tr><tr><td>4100</td><td>1</td></tr><tr><td>4200</td><td>2</td></tr><tr><td>4300</td><td>2</td></tr><tr><td>4400</td><td>2</td></tr><tr><td>4600</td><td>5</td></tr><tr><td>4700</td><td>3</td></tr><tr><td>4800</td><td>3</td></tr><tr><td>5000</td><td>1</td></tr><tr><td>5100</td><td>1</td></tr><tr><td>5200</td><td>1</td></tr><tr><td>5400</td><td>1</td></tr><tr><td>5600</td><td>1</td></tr><tr><td>5700</td><td>1</td></tr><tr><td>5800</td><td>1</td></tr><tr><td>5900</td><td>2</td></tr><tr><td>6000</td><td>2</td></tr><tr><td>6700</td><td>1</td></tr><tr><td>6900</td><td>1</td></tr><tr><td>7700</td><td>1</td></tr><tr><td>9600</td><td>1</td></tr><tr><td>10400</td><td>1</td></tr><tr><td>11700</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         122809
        ],
        [
         100,
         2548
        ],
        [
         200,
         1032
        ],
        [
         300,
         557
        ],
        [
         400,
         393
        ],
        [
         500,
         276
        ],
        [
         600,
         178
        ],
        [
         700,
         148
        ],
        [
         800,
         140
        ],
        [
         900,
         92
        ],
        [
         1000,
         98
        ],
        [
         1100,
         71
        ],
        [
         1200,
         57
        ],
        [
         1300,
         49
        ],
        [
         1400,
         42
        ],
        [
         1500,
         36
        ],
        [
         1600,
         25
        ],
        [
         1700,
         29
        ],
        [
         1800,
         23
        ],
        [
         1900,
         23
        ],
        [
         2000,
         17
        ],
        [
         2100,
         12
        ],
        [
         2200,
         11
        ],
        [
         2300,
         11
        ],
        [
         2400,
         11
        ],
        [
         2500,
         12
        ],
        [
         2600,
         13
        ],
        [
         2700,
         6
        ],
        [
         2800,
         7
        ],
        [
         2900,
         5
        ],
        [
         3000,
         5
        ],
        [
         3100,
         3
        ],
        [
         3200,
         3
        ],
        [
         3300,
         5
        ],
        [
         3400,
         3
        ],
        [
         3500,
         3
        ],
        [
         3600,
         4
        ],
        [
         3700,
         6
        ],
        [
         3800,
         1
        ],
        [
         3900,
         2
        ],
        [
         4000,
         3
        ],
        [
         4100,
         1
        ],
        [
         4200,
         2
        ],
        [
         4300,
         2
        ],
        [
         4400,
         2
        ],
        [
         4600,
         5
        ],
        [
         4700,
         3
        ],
        [
         4800,
         3
        ],
        [
         5000,
         1
        ],
        [
         5100,
         1
        ],
        [
         5200,
         1
        ],
        [
         5400,
         1
        ],
        [
         5600,
         1
        ],
        [
         5700,
         1
        ],
        [
         5800,
         1
        ],
        [
         5900,
         2
        ],
        [
         6000,
         2
        ],
        [
         6700,
         1
        ],
        [
         6900,
         1
        ],
        [
         7700,
         1
        ],
        [
         9600,
         1
        ],
        [
         10400,
         1
        ],
        [
         11700,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "rating_bin",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql WITH q AS (SELECT * FROM rating_distribution_playAndPurchase) SELECT `rating_bin`,SUM(`count`) `column_94f965f5311` FROM q GROUP BY `rating_bin`",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "rating_bin",
             "id": "column_94f965f5310"
            },
            "y": [
             {
              "column": "count",
              "id": "column_94f965f5311",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_94f965f5311": {
             "name": "count",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "implicitDf": true,
        "rowLimit": 10000
       },
       "nuid": "5be9bf9b-f0d2-4705-87c5-8dcc81216143",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 2.7496566772460938,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "rating_bin",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "rating_bin",
           "type": "column"
          },
          {
           "alias": "column_94f965f5311",
           "args": [
            {
             "column": "count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Query the binned playtime distribution from the temporary view to show how many user-game interactions fall into each 100-hour rating bucket.\n",
    "-- The result table is used to produce a bar chart for visualisation\n",
    "SELECT * FROM rating_distribution_playAndPurchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "502c8e9e-cb26-4f78-b91e-2f5c87870077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95th percentile - Play Only: 152.0\n95th percentile - Play and Purchase (only play values): 72.0\n"
     ]
    }
   ],
   "source": [
    "# Get the 95th percentile of playtime (ratings > 1) in each model\n",
    "\n",
    "# For Play Only rating dataframe\n",
    "percentile_95_play = steamDF_playOnly.approxQuantile(\"rating\", [0.95], 0.01)[0]\n",
    "\n",
    "# For Play and Purchase rating dataframe (only playtime rows)\n",
    "percentile_95_playPurchase = steamDF_playAndPurchase.filter(\"rating > 1.0\").approxQuantile(\"rating\", [0.95], 0.01)[0]\n",
    "\n",
    "print(f\"95th percentile - Play Only: {percentile_95_play}\")\n",
    "print(f\"95th percentile - Play and Purchase (only play values): {percentile_95_playPurchase}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9733f871-55d6-4e5f-8126-1afde067b121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Capping steamDF_playOnly to 95% percentile\n",
    "\n",
    "steamDF_playOnly_capped = steamDF_playOnly.withColumn(\"rating\",when(col(\"rating\") > percentile_95_play, percentile_95_play).otherwise(col(\"rating\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc038eb6-891c-47ad-a48a-6f037c1c63ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings summary - play only / After capping:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>summary</th><th>rating</th></tr></thead><tbody><tr><td>count</td><td>70,477.00</td></tr><tr><td>mean</td><td>22.26</td></tr><tr><td>stddev</td><td>40.49</td></tr><tr><td>min</td><td>0.10</td></tr><tr><td>max</td><td>152.00</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "count",
         "70,477.00"
        ],
        [
         "mean",
         "22.26"
        ],
        [
         "stddev",
         "40.49"
        ],
        [
         "min",
         "0.10"
        ],
        [
         "max",
         "152.00"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "summary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rating",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display summary statistics to confirm the effect of the capping\n",
    "print(\"Ratings summary - play only / After capping:\")\n",
    "steamDF_playOnly_capped.describe(\"rating\").withColumn(\"rating\",format_number(col(\"rating\").cast(\"double\"), 2)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "280d85f0-99c1-43e5-83a7-7642ab7af8a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Capping steamDF_playAndPurchase to 95% percentile\n",
    "\n",
    "# Cap played_hours before computing the combined rating\n",
    "steamDF_playAndPurchase_capped = pivotedDF.withColumn(\"played_hours\", when(col(\"played_hours\") > percentile_95_playPurchase, percentile_95_playPurchase).otherwise(col(\"played_hours\")))\n",
    "\n",
    "# Recalculate final rating after capping playtime\n",
    "steamDF_playAndPurchase_capped = steamDF_playAndPurchase_capped.withColumn(\"rating\", col(\"played_hours\") + (col(\"purchased_flag\") * purchase_weight)).select(\"memberIndex\", \"gameIndex\", \"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89c5f1d7-4912-4914-8681-705de5ca3837",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings summary - play and purchase / After capping:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>summary</th><th>rating</th></tr></thead><tbody><tr><td>count</td><td>128,804.00</td></tr><tr><td>mean</td><td>13.80</td></tr><tr><td>stddev</td><td>18.97</td></tr><tr><td>min</td><td>5.00</td></tr><tr><td>max</td><td>77.00</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "count",
         "128,804.00"
        ],
        [
         "mean",
         "13.80"
        ],
        [
         "stddev",
         "18.97"
        ],
        [
         "min",
         "5.00"
        ],
        [
         "max",
         "77.00"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "summary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rating",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display summary statistics to confirm the effect of the capping\n",
    "print(\"Ratings summary - play and purchase / After capping:\")\n",
    "steamDF_playAndPurchase_capped.describe(\"rating\").withColumn(\"rating\",format_number(col(\"rating\").cast(\"double\"), 2)).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a26ae0-e448-43dd-9946-8108f0955f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "During the pre processing stage, the inspection of the preprocessed data confirmed that the pivoting of behaviour values  produced the intended structure with one row per member/game pair and separate columns for played hours and  purchase. \n",
    "\n",
    "Similarly, the step to ensure purchases were always assigned a value of 1.0 was also confirmed in the preview, where interactions involving purchase behaviour appeared with played_hours = 0 and purchased_flag = 1.\n",
    "\n",
    "The transformation of memberID and gameName into the numerical memberIndex and gameIndex columns using  the StringIndexer was also successful, as shown in the the schema output confirming both columns were properly casted to integer to conform with the ALS algorithm requirements.\n",
    "\n",
    "The visualisation of samples from the first prepared DataFrame, steamDF_playOnly,  included only hours played interactions with non-zero durations, while the second DataFrame, steamDF_playAndPurchase, sample visualisations confirmed the combination of both playtime and purchase information into a single rating, consequently confirming that the two DataFrames could be used as input for model training under two different feedback assumptions.\n",
    "\n",
    "The steamDF_playOnly,  statistics summary showed 70,477 interactions with non-zero playtime, and a highly skewed distribution of rating values with an average of 48.89 hours and a maximum of 11,754 hours. The corresponding bar chart confirmed a typical long-tail distribution, where most user-game interactions were in the 0–100 hours range, while a small number of member/game interactions involved very high accumulated play hours.\n",
    "\n",
    "The statistics summary of the steamDF_playAndPurchase DataFrame included 128,804 member-game pairs, combining playtime and purchase indicators into a single implicit feedback score, the inclusion of the purchase interaction, in this case with a weight value of five,  as expected, caused the mean to drop to 31.75, but the data was still right-skewed again due to long-duration play interactions, as confirmed by the bar chart of 100-binned rating distribution.\n",
    "\n",
    "To address the risk of overfitting from high outlier rating values, the 95th percentile of playtime ratings was calculated, 152.0 for steamDF_playOnly, and 72.0 for the play component of steamDF_playAndPurchase and these values were then used to cap ratings in both DataFrames, reducing the influence of outliers above these thresholds.\n",
    "\n",
    "The effect of capping was confirmed by generating a summary of statistics for each DataFrame, showing that the mean and standard deviation were considerably reduced. For example, the average rating in the steamDF_playAndPurchase dataset dropped from 31.75 to 13.80, and its standard deviation went from 171.39 to 18.97,  showing a substantial reduction in the effect of extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75f4e3c1-d831-4af2-a1d6-3923e246653c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95a9edfd-5b26-4f5b-88eb-03146a9ed301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f700c268-4004-4caa-b699-196703c6e641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Model Training##\n",
    "\n",
    "We began this stage by splitting the preprocessed and capped datasets into training and testing sets using an 80/20 ratio and setting a fixed seed (seed=42) to ensure reproducibility. This is performed independently for the two datasets prepared.\n",
    "\n",
    "To train the models, we define a reusable function `train_baseline_model()` that logs parameters and performance metrics using MLflow and returns the trained ALS model along with its predictions. The function signature includes three parameters intended to receive as arguments the corresponding training and testing sets as well as a model label String to identify the training dataset the model was trained on. Inside the function, we configured a baseline ALS (Alternating Least Squares) model using the following default hyperparameters:\n",
    "\n",
    "- rank=10: Number of latent features.\n",
    "\n",
    "- regParam=0.1: Regularisation to prevent overfitting.\n",
    " \n",
    "- maxIter=10: Maximum number of alternating optimisation steps.\n",
    " \n",
    "- implicitPrefs=True: Indicates the use of implicit feedback.\n",
    " \n",
    "- coldStartStrategy=\"drop\": Ensures that predictions are evaluated only for games and users seen during training.\n",
    "\n",
    "In the function, the fit() method takes the training data set as input to train the ALS model. Upon receiving the input DataFrame containing the list of known member/games interaction in the testing set, the ALS algorithm implementation in Spark MLlib internally carries out initial verification of data types and formats. Then, the DataFrame is converted into an indexed sparse matrix format, mapping each member to the games they have actually played along with the corresponding rating (representing either the number of hours played or the number of hours played and the purchase behaviour, depending on the feature engineering approach for the input dataset) and each game to each of the members that have played that game and the corresponding rating, effectively compactifying the input DataFrame by not including the instances of null member/game interactions in the input DataFrame.\n",
    "\n",
    "Next, a vector is initialised for each member and game. These vectors are lists of, initialy random, n values, where n is given by the Rank hyperparameter. Each value in a given position within a game vector represents the level of association of the game with a latent game feature, while the corresponding value within a member vector represents the member's level of preference or how much the member cares about the corresponding latent game feature in the same position. These game features are in a hidden state as their association with real game features such as \"action genre\" or \"made by Activision\" is not explicit. Consequently a game vector indicates the relative importance of game features associated to a game and, likewise, a member vector indicates the relative importance of the member's preferences for those game features. Therefore, the dot product between a member vector and a game vector, the degree of alignment between vectors, tends to be higher when the relative importance across features is similar, indicating a high affinity and vice versa. Altogether, the set of game and member vectors can be thought of as occupying an n-dimensional feature space. Each of the values within a vector indicates the vector position in an n-axis in the n-dimensional feature space and together represent the location or coordinates of the game or member in the same space. \n",
    "\n",
    "Then, each possible game/member combination is given a preference value. A preference value of 1 denotes that there is a corresponding rating within the sparse matrix, indicating an interaction between a user and a game, and a value of zero denotes that there is no rating associated with a particular user/member combination in the sparse matrix, indicating the absence of interaction.\n",
    "\n",
    "The goal of ALS is to find values for user and game vectors in the training set such that the dot product between them approximates an affinity score, a real number value, that is closer to 1 for member/game pairs with preference value of 1 and closer to zero for member/game pairs with preference value of zero. This is achieved by adjusting the vector values via computing a loss function accross all user-game combinations as the squared difference between the predicted affinity score (the dot product of the user and game vectors) and the preference value associated to the member/game pair, multiplied by a confidence value for that interaction that allows the model to give more emphasis to stronger interactions (larger rating).  Finally, a regularisation term is added to the loss function to prevent overfitting by discouraging excessively large values in the vectors.\n",
    "\n",
    "The confidence value is used in the loss function because the presence or absence of interaction alone does not capture the strength of the member's preference to purchase or purchase and play a game, hence a confidence value is associated to each member/game combination to assign them weights, indicating the strength of a member/game interaction to indicate a preference. The value is calculated by adding the preference value to the rating value scaled by multiplying it by a fixed alpha value with the purpose of emphasising and magnifying the magnitude of the rating. ​Consequently, the confidence value encapsulates the member's preference for a game, indicating the presence or absence of purchase or purchase and play behaviour, depending on the feature engineering approach in our models, and the strength of that preference. The confidence value expresses how strongly the algorithm should trust an observed interaction. \n",
    "\n",
    "The regularisation term is added to the loss function to discourage the vector values from growing too large and to help prevent overfitting. This term has a penalisation effect whose extent is the result of multiplying the regParam hyperparameter by the result of adding the sum of the squared values in the member vector and the sum of the squared values in the game vector. The value of the regParam determines the magnitude of the penalisation effect of the regularisation term in the loss function. \n",
    "\n",
    "To minimise the loss function mentioned above, the ALS algorithm factorises the user-game interaction matrix into two sets of vectors: one for users (members) and one for games. It then uses an alternating least squares approach to learn the values of these vectors. On each iteration, it fixes the values in one set of vectors (either member or game) and solves for the other. This optimisation is repeated over several iterations, as specified by the maxIter hyperparameter, progressively improving the estimates of both sets of vectors. When solving for a specific vector (for example, a game vector), the algorithm considers all users who interacted with that game and calculates the dot product between each of those fixed user vectors and the unknown game vector. Ideally, this dot product would exactly match the preference value for each of those user-game interactions. However, since there is no single vector that can produce a perfect dot product for every case, the algorithm instead looks for a compromise by finding values for the unknown vector that minimises the total confidence-weighted squared error across all of those users, which is exactly what the loss function represents. \n",
    "\n",
    "To compute the values for each vector during optimisation, ALS solves for one unknown vector at a time. For each, a system of linear equations is derived by taking the partial derivatives of the loss function with respect to each unknown element, setting them to zero, and solving the resulting system using matrix operations. Once all vectors have been updated for the current side (either games or members), the roles are reversed and the process continues on the other side. This cycle repeats until the algorithm converges or reaches the maximum number of iterations.\n",
    "\n",
    "After the trainig data set is fitted to the model inside the function, the trained model is used to generated predicitons (ratings) on the testing set with the `transform()` method. These predictions were later used for the evaluation of the model.\n",
    "\n",
    "The function returns the trained model and its label and predictions on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b0ce74-a62c-42cf-a592-c85c0bc105df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play Only Model:\nTraining set size: 56613\nTest set size: 13864\n\nPlay and Purchase Model:\nTraining set size: 103168\nTest set size: 25636\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into training and test sets\n",
    "# Splits 80% training, 20% testing\n",
    "# seed=42 Ensures the same split each time for reproducibility\n",
    "\n",
    "# For model using only 'play' behaviour\n",
    "train_playOnly, test_playOnly = steamDF_playOnly_capped.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Play Only Model:\")\n",
    "print(f\"Training set size: {train_playOnly.count()}\")\n",
    "print(f\"Test set size: {test_playOnly.count()}\")\n",
    "\n",
    "# For model using both 'play' and 'purchase' behaviours\n",
    "train_playAndPurchase, test_playAndPurchase = steamDF_playAndPurchase_capped.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"\\nPlay and Purchase Model:\")\n",
    "print(f\"Training set size: {train_playAndPurchase.count()}\")\n",
    "print(f\"Test set size: {test_playAndPurchase.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7488dee-0840-44cd-ab6b-659174f5cd70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to train and log to MLflow a baseline model, returning the model, its label and predictions\n",
    "# model_label allows to recognise whether a model was trained on play only or play and purchase data\n",
    "def train_baseline_model(train_df, test_df, model_label):\n",
    "     #Starts and names a run in the MLflow experiment\n",
    "    with mlflow.start_run(run_name=f\"Training_{model_label}_Baseline\"):\n",
    "        # Log the size of the training and test datasets to MLflow\n",
    "        mlflow.log_param(\"train_size\", train_df.count())\n",
    "        mlflow.log_param(\"test_size\", test_df.count())\n",
    "        \n",
    "        # Print a message to indicate which model is being trained with default ALS hyperparameters\n",
    "        print(f\"\\nTraining {model_label} with default ALS hyperparameters\")\n",
    "\n",
    "        # Define the ALS model\n",
    "        als = ALS(\n",
    "            userCol=\"memberIndex\", # Column representing the user (member) id\n",
    "            itemCol=\"gameIndex\", # Column representing the item (game) id\n",
    "            ratingCol=\"rating\", # Column representing the user/item  (member/game) interaction (implicit feedback)\n",
    "            implicitPrefs=True, # True = Implicit feedback\n",
    "            coldStartStrategy=\"drop\", # Drops NaNs during evaluation on testing set (due to unseen users/items during testing)\n",
    "            rank=10, # Number of latent factors (dimensionality of user/item feature vectors)\n",
    "            regParam=0.1, # Regularisation to prevent overfitting\n",
    "            maxIter=10 # Number of training iterations\n",
    "        )\n",
    "\n",
    "        # Train the model (fit the model on the training set)\n",
    "        model = als.fit(train_df)\n",
    "\n",
    "        # Make predictions (of ratings) on the test set\n",
    "        predictions = model.transform(test_df)\n",
    "\n",
    "        # Logs the trained model to MLflow tracking (Allow to reload the model if needed later)\n",
    "        mlflow.spark.log_model(model, f\"{model_label}_baseline_model\")\n",
    "\n",
    "        # Log the trained model's configuration and label to MLflow\n",
    "        mlflow.log_param(\"rank\", 10)\n",
    "        mlflow.log_param(\"regParam\", 0.1)\n",
    "        mlflow.log_param(\"maxIter\", 10)\n",
    "        mlflow.log_param(\"implicitPrefs\", True)\n",
    "        mlflow.log_param(\"model\", model_label)\n",
    "        \n",
    "\n",
    "    # Returns a dictionary with the model, its label and predictions\n",
    "    return {\"model_label\": model_label, \"model\": model, \"predictions\": predictions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15dbb5a9-c1ca-4ee1-9dc3-b28310764d86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTraining Model_PlayOnly with default ALS hyperparameters\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "# Baseline model using Play only\n",
    "# model_label allows to recognise whether a model was trained on play only or play and purchase data\n",
    "baseline_model_playOnly = train_baseline_model(train_playOnly, test_playOnly, model_label=\"Model_PlayOnly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c98b1d03-6528-465e-84f4-735f5b9e5b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTraining Model_PlayAndPurchase with default ALS hyperparameters\n"
     ]
    }
   ],
   "source": [
    "# Baseline model using Play and Purchase\n",
    "# model_label allows to recognise whether a model was trained on play only or play and purchase data\n",
    "baseline_model_playAndPurchase = train_baseline_model(train_playAndPurchase, test_playAndPurchase, model_label=\"Model_PlayAndPurchase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec9c01e2-12f3-43d8-8275-c984a2347af6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The data split yielded 56,613 training member/game interactions and 13,864 test member/game interactions from the dataset for the training using hours played as implicit feedback and  103,168 training member/game interactions and 25,636 test member/game interactions from the dataset for the training using hours played and purchase behaviour as implicit feedback, confirming that the full datasets were correctly split without loss and that we had a substantial volume of data for training both models.\n",
    "\n",
    "Subsequently, the training process of both baseline models was completed successfully using default ALS hyperparameters, and predictions were generated on their respective test sets. \n",
    "\n",
    "At this stage, the models have learned the latent vectors for each member and game, with embedded preferences influenced by either the hours played or the combined purchase behaviour and hours played and predictions of the trained models on their corresponding test data sets are ready to be used for model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3525106c-0ecf-4b92-b143-4c31b54846d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c6aafcc-116d-43e1-863a-b43dd8cc5d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b165cc2f-1890-4c29-96c1-c8092d206647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Hyperparameter Tunning##\n",
    "\n",
    "In this stage, we developed the 'train_als_with_gridsearch' function which replicates the  'train_baseline_model' but additionally implements a hyperparameter grid intending to improve the baseline performance of the ALS models by tuning their hyperparameters through a grid search approach. ALS performance can be sensitive to the configuration of certain hyperparameters, particularly rank, which determines the number of latent features, and regParam, which controls the degree of regularisation to prevent overfitting. Therefore, instead of relying on default values, the function uses a Python nested loop control flow structure to iterate over all the possible combinations of a set of rank values and regParam values. maxIter is fixed at 10 to limit training time while allowing the models to converge adequately. All the model objects generated and their, configurations, predictions on their test data sets and labels are saved in the `all_models_info = []` list and the list is returned for further evaluation and comparison of models. The label saved to the list for each model is a constructed an unique label so that the model can be identified not only by the data it was trained on but also the hyperparameters used for hypertuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36e166dc-daa1-4ef3-b678-26dfdcbda8ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to train and log a model with hyperparameter tunning, returning the models, their labels and predictions\n",
    "# Loops through combinations of rank and regParam. Tracks all model using MLflow, logging all relevant hyperparameters\n",
    "def train_als_with_gridsearch(train_df, test_df, model_label):\n",
    "    # Define parameter grid for gridsearch (hyperparameter tuning)\n",
    "    ranks = [5, 15]\n",
    "    regParams = [0.001, 0.01, 0.05]\n",
    "    maxIter = 10\n",
    "\n",
    "    # List to store all the models and their predictions\n",
    "    all_models_info = []\n",
    "\n",
    "    # Grid Search Loop iterates over each combination of hyperparameters to train the model\n",
    "    for rank in ranks:\n",
    "        for reg in regParams:\n",
    "            # Uses the model label and hyperparameter values to construct a name for the run\n",
    "            run_name = f\"Training_{model_label}_rank{rank}_reg{reg}_iter{maxIter}\"\n",
    "            # Starts and names (using the constructed name) a run in the MLflow experiment for this loop iteration\n",
    "            with mlflow.start_run(run_name=run_name):\n",
    "                \n",
    "                # Log the size of the training and test datasets to MLflow\n",
    "                mlflow.log_param(\"train_size\", train_df.count())\n",
    "                mlflow.log_param(\"test_size\", test_df.count())\n",
    "\n",
    "                # Display current iteration model and hyperparameters\n",
    "                print(f\"\\nTraining {model_label} with rank={rank}, regParam={reg}, maxIter={maxIter}\")\n",
    "\n",
    "                # Define the ALS model\n",
    "                als = ALS(\n",
    "                    userCol=\"memberIndex\", # Column representing the user (member) id\n",
    "                    itemCol=\"gameIndex\", # Column representing the item (game) id\n",
    "                    ratingCol=\"rating\", # Column representing the user-item interaction (implicit feedback)\n",
    "                    implicitPrefs=True, # True = Implicit feedback\n",
    "                    coldStartStrategy=\"drop\", # Drops NaNs during evaluation on testing set (due to users/games only present in the testing set so not seen during training)\n",
    "                    rank=rank, # Number of latent factors (dimensionality of user/item feature vectors)\n",
    "                    regParam=reg, # Regularisation to prevent overfitting\n",
    "                    maxIter=maxIter # Number of training iterations\n",
    "                )\n",
    "\n",
    "                # Train the model (fit the model on the training set)\n",
    "                model = als.fit(train_df)\n",
    "\n",
    "                # Make predictions (ratings) on the test set \n",
    "                predictions = model.transform(test_df)\n",
    "\n",
    "                # Logs the trained model to MLflow trackin (Allow to reload the model if needed later)\n",
    "                mlflow.spark.log_model(model, f\"{model_label}_rank{rank}_reg{reg}_model\")\n",
    "\n",
    "                # Appends the trained model's metadata to all_models_info list\n",
    "                all_models_info.append({\n",
    "                    # This constructs a unique label so that the model can be identified not only by the data it was trained on but also the hyperparameters used for hypertuning.\n",
    "                    \"model_label\": f\"{model_label}_rank{rank}_reg{reg}\",\n",
    "                    \"rank\": rank,\n",
    "                    \"regParam\": reg,\n",
    "                    \"model\": model,\n",
    "                    \"predictions\": predictions\n",
    "                })\n",
    "\n",
    "                # Log current iteration's model configuration and label to MLflow\n",
    "                mlflow.log_param(\"model\", model_label)\n",
    "                mlflow.log_param(\"rank\", rank)\n",
    "                mlflow.log_param(\"regParam\", reg)\n",
    "                mlflow.log_param(\"maxIter\", maxIter)\n",
    "                mlflow.log_param(\"implicitPrefs\", True)\n",
    "\n",
    "    # Return the list with all the models and their predictions\n",
    "    return all_models_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "204f2235-8485-419a-8ba4-10db37514467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTraining Model_PlayOnly with rank=5, regParam=0.001, maxIter=10\n\nTraining Model_PlayOnly with rank=5, regParam=0.01, maxIter=10\n\nTraining Model_PlayOnly with rank=5, regParam=0.05, maxIter=10\n\nTraining Model_PlayOnly with rank=15, regParam=0.001, maxIter=10\n\nTraining Model_PlayOnly with rank=15, regParam=0.01, maxIter=10\n\nTraining Model_PlayOnly with rank=15, regParam=0.05, maxIter=10\n"
     ]
    }
   ],
   "source": [
    "# Play only models with hyperparameter tuning (returns list of models dictionary)\n",
    "# model_label allows to recognise whether a model was trained on play only or play and purchase data\n",
    "tuned_models_playOnly = train_als_with_gridsearch(train_playOnly, test_playOnly, model_label=\"Model_PlayOnly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b95b7697-446f-4507-9830-c401f799bc29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTraining Model_PlayAndPurchase with rank=5, regParam=0.001, maxIter=10\n\nTraining Model_PlayAndPurchase with rank=5, regParam=0.01, maxIter=10\n\nTraining Model_PlayAndPurchase with rank=5, regParam=0.05, maxIter=10\n\nTraining Model_PlayAndPurchase with rank=15, regParam=0.001, maxIter=10\n\nTraining Model_PlayAndPurchase with rank=15, regParam=0.01, maxIter=10\n\nTraining Model_PlayAndPurchase with rank=15, regParam=0.05, maxIter=10\n"
     ]
    }
   ],
   "source": [
    "# Play and Purchase models with hyperparameter tuning (returns list of models dictionary)\n",
    "# model_label allows to recognise whether a model was trained on play only or play and purchase data\n",
    "tuned_models_playAndPurchase = train_als_with_gridsearch(train_playAndPurchase, test_playAndPurchase, model_label=\"Model_PlayAndPurchase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42d84b97-00a8-46c7-b915-5b2a14795e95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The tuning procedure was completed successfully for both model variations, one using hours played and the other using purchase and hours played as implicit feedback, producing twelve tuned models each (6 for each model variation) representing different combinations of latent space feature dimensions via the values in the rank hyperparameter with different levels of regularisation to prevent overfitting via the regParam hyperparameter. At this point all these models and their, configurations, predictions on their test data sets and labels saved in a list were available for further evaluation and comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a887e156-0abe-4c8c-847c-824437040027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4776c774-306c-4032-a71c-7b18c0b91858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f92a09e-3d64-40fe-bf15-29a003ca0990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Model Evaluation##\n",
    "\n",
    "After the training stage, we proceeded to evaluate all the trained ALS models, both baseline and hyperparameter-tuned, using three complementary metrics: Root Mean Squared Error (RMSE), Precision@10, and Recall@10. \n",
    "\n",
    "RMSE evaluates the prediction errors between actual and predicted ratings using the predictions made by the model on the test set. \n",
    "\n",
    "The RMSE metric is useful for tracking relative model improvement across different hyperparameters configurations, therefore we implemented the function `evaluate_rmse(model, model_label, predictions)` that uses an instance of the Spark's RegressionEvaluator object to calculate this metric. Internally, the metric is calculated \n",
    "as the square root of the mean squared differences between the model's predicted rating and actual rating across the member/game interactions in the test set. The\n",
    "differences are squared to penalise larger errors more heavily, since squaring amplifies the impact of larger differences more than smaller ones. Taking the square root then brings the error back to the same scale as the target variable, making it easier to interpret. Given that RMSE evaluates rating prediction accuracy but does not capture the quality of recommendations, we also produced Recall@N and Precision@N metrics to evaluate the quality of ten recommendations made by each model.\n",
    "\n",
    "Precision@N assesses the relevance of the recommendations made by the model to members on the testing set. It meassures the proportion of games that were actually played by the user with respect to the number of recommendations, reflecting the model's ability to prioritise meaningful recommendations. In the code implementation, the function `evaluate_precision(model, test_df, model_label, n):` is used to calculate the Precision@10 metric for each of the models generated. This function, takes for input a model object, its corresponding training set and a label that allows to identify within the function whether the model is a play or play and purchase infered feedback model. The function then defines an inner function `precision_udf(actual, predicted)` that takes N predicted ratings made by the model for a member and the corresponding actual ratings recorded in the testing set and compares them calculating precision as ratio of relevant recommendations to total recommendations. The `precision_udf(actual, predicted)` function is wrapped up as a Spark User Defined Function (UDF) in ` precision_udf_spark = udf(precision_udf, DoubleType())` so it can be used with Spark Dataframes and executed in a distributed manner, returning the precision metric as a double data type value. An if-else conditional flow control structure was used within the function to select the correct testing set of the model being evaluated, extract the members included in the training set and intersect them with the corresponding testing set so that the evaluation of precision is carried out using predicitions made by the model on the testing set involving only members seen during the model's training, given the impossibility of an ALS trained model to generate correctly recommendations for users (members) not seen during its training. This was done out of an excess of caution given the size of the datasets involved. A helper function `get_top_n_recommendations(model, users_df, n)`, which implements Spark's `recommendForUserSubset` method, was used to generate the N recommendations for the members. \n",
    "\n",
    "Recall@10 measures the coverage of the recommendations made by the model to members on the testing set. It meassures the proportion of games played by the member that are in the generated recommendations with respect to the total number of games played by the member. A process similar to the calculation of Precision@N implemented in the function `evaluate_precision(model, test_df, model_label, n):` was implemented in the function `evaluate_recall(model, test_df, model_label, n):` to evaluate Recall@10 for each model.\n",
    "       \n",
    "Then the `evaluate_rmse`, `evaluate_precision` and `evaluate_recall` functions are invoked passing all the baseline and hypertuned models. The `evaluation_results` list saves all the model metrics in the process and a function `log_evaluation_to_mlflow(model_label, rmse, precision, recall)` starts experiment runs in MLflow and logs all the metrics for the models evaluated. Subsequently, the `evaluation_results` list is used to produce a DataFrame which is then displayed showing all the models' metrics ordered by ascending RMSE. Lastly, this DataFrame is ordered by presicion descending order and then by recall descending order to retrieve the first row of each resulting DataFrame and extract the model labels corresponding to the model's with best precision and best recall, respectively and then use these labels to retrieve the corresponding model objects and store them in the best_precision_model and best_recall_model variables in preparaction for generating recommendations and comparison. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5329944b-0a8b-427d-b22e-270604ca9396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate Root Mean Square Error (RMSE)\n",
    "# RMSE is the square root of the mean squared differences between model's predicted rating and actual rating across the member/game interactions in the test set\n",
    "#  Differences are squared to penalise larger errors more heavily, as squaring amplifies the impact of larger differences more than smaller ones. The square root then returns the mean of the squared differences to the original scale of the target variable, making it easier to interpret.\n",
    "# Function to compute RMSE\n",
    "def evaluate_rmse(model, model_label, predictions):\n",
    "    \n",
    "    print(f\"Evaluating RMSE for {model_label}\")\n",
    "    \n",
    "    # Initialise evaluator to evaluate the model using RMSE\n",
    "    evaluator = RegressionEvaluator(\n",
    "        metricName=\"rmse\",\n",
    "        labelCol=\"rating\", # Actual rating values in the training set\n",
    "        predictionCol=\"prediction\" # Model predicted ratings in the training set\n",
    "    )\n",
    "    # Compare predictions vs actual ratings in the test set to compute the RMSE\n",
    "    # RMSE measures how close the predicted rating (hours played or 1 for purchase) was to the actual rating in the testing set\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb4cf747-8bc6-4b09-89ca-5cb7b32ff65c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Custom function to, given a model, compute Top_N recomendations for all users\n",
    "def get_top_n_recommendations(model, users_df, n):\n",
    "    # Uses spark recommendForUserSubset function to get the n recommendations\n",
    "    recommendations = model.recommendForUserSubset(users_df, n)\n",
    "    # Flattens the nested list containing the top-N recommendations per user so that each row \n",
    "    # contains a single (user, game) recommendation along with its predicted rating.\n",
    "    return recommendations.withColumn(\"rec_exp\", explode(\"recommendations\")).select(\n",
    "        col(\"memberIndex\"),\n",
    "        col(\"rec_exp.gameIndex\").alias(\"gameIndex\"),\n",
    "        col(\"rec_exp.rating\").alias(\"predicted_rating\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23381d75-5fab-4708-8d65-bd15db37cc0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluates Precision@N for a given model and test dataset, returning the average precision across all users.\n",
    "# Precision@N measures how many of the top n games recommended to a user were actually played or purchased by the user in the test set (Relevance of recommendations)\n",
    "def evaluate_precision(model, test_df, model_label, n):\n",
    "\n",
    "    print(f\"Evaluating Precision@{n} for {model_label}\")\n",
    "\n",
    "    # Defines a function to calculate precision for a single user by comparing actual vs. predicted game sets\n",
    "    def precision_udf(actual, predicted):\n",
    "        #Convert actual interactions to a set\n",
    "        actual_set = set(actual)\n",
    "        # Convert predicted interactions to a set\n",
    "        predicted_set = set(predicted)\n",
    "        # Count intersection between actual and predicted sets\n",
    "        true_positives = len(actual_set & predicted_set)\n",
    "        # Calculates precision as ratio of relevant recommendations to total recommendations\n",
    "        precision = float(true_positives) / len(predicted_set) if predicted_set else 0.0\n",
    "        # Return precision\n",
    "        return precision\n",
    "\n",
    "    # Converts the precision_udf function to a Spark User Defined Function so it can be used in DataFrame transformations\n",
    "    precision_udf_spark = udf(precision_udf, DoubleType())\n",
    "\n",
    "\n",
    "    # Set of users in the training set that have been seen during training (Avoids making suggestions on members not seen during trainig)\n",
    "    # Uses the model_label to identify the training set used to train the model\n",
    "    #If an instance of the substring \"PlayOnly\" is found in the label\n",
    "    if \"PlayOnly\" in model_label: \n",
    "        # Distinct members in the train_playOnly set\n",
    "        training_users_df = train_playOnly.select(\"memberIndex\").distinct()\n",
    "    else:\n",
    "        # Distinct members in the train_playAndPurchase set\n",
    "        training_users_df = train_playAndPurchase.select(\"memberIndex\").distinct()\n",
    "    # Distinct members in the test\n",
    "    test_users_df = test_df.select(\"memberIndex\").distinct()\n",
    "    # Distinct members in the test set who are also in the training set\n",
    "    users_df = training_users_df.intersect(test_users_df)\n",
    "    \n",
    "\n",
    "    # Get top-N recommendations for each distinct member in the test set who is also in the trainig set (Uses the custom function)\n",
    "    recommendations = get_top_n_recommendations(model, users_df, n)\n",
    "\n",
    "    # Get the actual games the members interacted with\n",
    "    actual_df = test_df.select(\"memberIndex\", \"gameIndex\").groupBy(\"memberIndex\").agg(collect_set(\"gameIndex\").alias(\"actual_games\"))\n",
    "\n",
    "    # Aggregates the Top-10 recommendations generated for each user into a set for comparison\n",
    "    predicted_df = recommendations.select(\"memberIndex\", \"gameIndex\").groupBy(\"memberIndex\").agg(collect_set(\"gameIndex\").alias(\"predicted_games\"))\n",
    "\n",
    "    # Join both actual and predicted for comparison\n",
    "    comparison_df = actual_df.join(predicted_df, on=\"memberIndex\", how=\"inner\")\n",
    "\n",
    "    # Applies the precision UDF to each row in the comparison Dataframe\n",
    "    result_df = comparison_df.withColumn(\"precision\", precision_udf_spark(\"actual_games\", \"predicted_games\"))\n",
    "\n",
    "    # Computes the average precision across all users\n",
    "    avg_precision = result_df.agg({\"precision\": \"avg\"}).collect()[0][\"avg(precision)\"]\n",
    "\n",
    "    # Returns average precision\n",
    "    return avg_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1581d300-01de-4322-a471-8a9b0e205936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluates Recall@N for a given model and test dataset, returning the average recall across all users.\n",
    "# Recall@N measures how many of the games actually played by a user in the test set were found in the top n recommendations made by the model (Coverage of recommendations)\n",
    "def evaluate_recall(model, test_df, model_label, n):\n",
    "   \n",
    "    print(f\"Evaluating Recall@{n} for {model_label}\")\n",
    "\n",
    "    # Defines a function to calculate Recall@N for a single user based on actual and predicted game sets\n",
    "    def recall_udf(actual, predicted):\n",
    "        #Convert actual interactions to a set\n",
    "        actual_set = set(actual)\n",
    "        # Convert predicted interactions to a set\n",
    "        predicted_set = set(predicted)\n",
    "        # Count intersection between actual and predicted sets\n",
    "        true_positives = len(actual_set & predicted_set)\n",
    "        # Calculates recall as the ratio of relevant items retrieved (true positives) to all relevant items (actual set)\n",
    "        recall = float(true_positives) / len(actual_set) if actual_set else 0.0\n",
    "        # Return recall\n",
    "        return recall\n",
    "\n",
    "    # Converts the recall_udf function to a Spark User Defined Functio so it can be used in DataFrame transformations\n",
    "    recall_udf_spark = udf(recall_udf, DoubleType())\n",
    "\n",
    "    # Set of users in the training set that have been seen during training (Avoids making suggestions on members not seen during trainig)\n",
    "    # Uses the model_label to identify the training set used to train the model\n",
    "    #If an instance of the substring \"PlayOnly\" is found in the label\n",
    "    if \"PlayOnly\" in model_label:\n",
    "        # Distinct members in the train_playOnly set\n",
    "        training_users_df = train_playOnly.select(\"memberIndex\").distinct()\n",
    "    else:\n",
    "        # Distinct members in the train_playAndPurchase set\n",
    "        training_users_df = train_playAndPurchase.select(\"memberIndex\").distinct()\n",
    "    # Distinct members in the test\n",
    "    test_users_df = test_df.select(\"memberIndex\").distinct()\n",
    "    # Distinct members in the test set who are also in the training set\n",
    "    users_df = training_users_df.intersect(test_users_df)\n",
    "    \n",
    "    \n",
    "    # Get top-N recommendations for each distinct member in the test set who is also in the trainig set (Uses the custom function)\n",
    "    recommendations = get_top_n_recommendations(model, users_df, n)\n",
    "\n",
    "    # Get the actual games the members interacted with\n",
    "    actual_df = test_df.select(\"memberIndex\", \"gameIndex\").groupBy(\"memberIndex\").agg(collect_set(\"gameIndex\").alias(\"actual_games\"))\n",
    "\n",
    "    # Aggregates the Top-10 recommendations generated for each user into a set for comparison\n",
    "    predicted_df = recommendations.select(\"memberIndex\", \"gameIndex\").groupBy(\"memberIndex\").agg(collect_set(\"gameIndex\").alias(\"predicted_games\"))\n",
    "\n",
    "    # Join both actual and predicted for comparison\n",
    "    comparison_df = actual_df.join(predicted_df, on=\"memberIndex\", how=\"inner\")\n",
    "\n",
    "    # Applies the recall UDF to each row in the comparison Dataframe\n",
    "    result_df = comparison_df.withColumn(\"recall\", recall_udf_spark(\"actual_games\", \"predicted_games\"))\n",
    "\n",
    "    # Computes the average recall across all users\n",
    "    avg_recall = result_df.agg({\"recall\": \"avg\"}).collect()[0][\"avg(recall)\"]\n",
    "\n",
    "    # returns average recall\n",
    "    return avg_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a218ee4-59fc-4982-be45-503cc633d1c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to log RMSE, Precision@N, and Recall@N to MLflow for a given model label.\n",
    "def log_evaluation_to_mlflow(model_label, rmse, precision, recall):\n",
    "   # Starts and names (using the model label) a run in the MLflow experiment\n",
    "    with mlflow.start_run(run_name=f\"Evaluation_{model_label}\"):\n",
    "        # Log the model label and metrics in MLflow\n",
    "        mlflow.log_param(\"model\", model_label)\n",
    "        mlflow.log_metric(\"RMSE\", rmse)\n",
    "        mlflow.log_metric(\"Precision@10\", precision)\n",
    "        mlflow.log_metric(\"Recall@10\", recall)\n",
    "        # Display confirmation\n",
    "        print(f\"Evaluation metrics logged to MLflow for {model_label}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "573ecbc0-4f22-41e8-9060-aec81e9e5724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE for Model_PlayOnly\nEvaluating Precision@10 for Model_PlayOnly\nEvaluating Recall@10 for Model_PlayOnly\nEvaluation metrics logged to MLflow for Model_PlayOnly\n\nEvaluating RMSE for Model_PlayAndPurchase\nEvaluating Precision@10 for Model_PlayAndPurchase\nEvaluating Recall@10 for Model_PlayAndPurchase\nEvaluation metrics logged to MLflow for Model_PlayAndPurchase\n\nEvaluating RMSE for Model_PlayOnly_rank5_reg0.001\nEvaluating Precision@10 for Model_PlayOnly_rank5_reg0.001\nEvaluating Recall@10 for Model_PlayOnly_rank5_reg0.001\nEvaluation metrics logged to MLflow for Model_PlayOnly_rank5_reg0.001\n\nEvaluating RMSE for Model_PlayOnly_rank5_reg0.01\nEvaluating Precision@10 for Model_PlayOnly_rank5_reg0.01\nEvaluating Recall@10 for Model_PlayOnly_rank5_reg0.01\nEvaluation metrics logged to MLflow for Model_PlayOnly_rank5_reg0.01\n\nEvaluating RMSE for Model_PlayOnly_rank5_reg0.05\nEvaluating Precision@10 for Model_PlayOnly_rank5_reg0.05\nEvaluating Recall@10 for Model_PlayOnly_rank5_reg0.05\nEvaluation metrics logged to MLflow for Model_PlayOnly_rank5_reg0.05\n\nEvaluating RMSE for Model_PlayOnly_rank15_reg0.001\nEvaluating Precision@10 for Model_PlayOnly_rank15_reg0.001\nEvaluating Recall@10 for Model_PlayOnly_rank15_reg0.001\nEvaluation metrics logged to MLflow for Model_PlayOnly_rank15_reg0.001\n\nEvaluating RMSE for Model_PlayOnly_rank15_reg0.01\nEvaluating Precision@10 for Model_PlayOnly_rank15_reg0.01\nEvaluating Recall@10 for Model_PlayOnly_rank15_reg0.01\nEvaluation metrics logged to MLflow for Model_PlayOnly_rank15_reg0.01\n\nEvaluating RMSE for Model_PlayOnly_rank15_reg0.05\nEvaluating Precision@10 for Model_PlayOnly_rank15_reg0.05\nEvaluating Recall@10 for Model_PlayOnly_rank15_reg0.05\nEvaluation metrics logged to MLflow for Model_PlayOnly_rank15_reg0.05\n\nEvaluating RMSE for Model_PlayAndPurchase_rank5_reg0.001\nEvaluating Precision@10 for Model_PlayAndPurchase_rank5_reg0.001\nEvaluating Recall@10 for Model_PlayAndPurchase_rank5_reg0.001\nEvaluation metrics logged to MLflow for Model_PlayAndPurchase_rank5_reg0.001\n\nEvaluating RMSE for Model_PlayAndPurchase_rank5_reg0.01\nEvaluating Precision@10 for Model_PlayAndPurchase_rank5_reg0.01\nEvaluating Recall@10 for Model_PlayAndPurchase_rank5_reg0.01\nEvaluation metrics logged to MLflow for Model_PlayAndPurchase_rank5_reg0.01\n\nEvaluating RMSE for Model_PlayAndPurchase_rank5_reg0.05\nEvaluating Precision@10 for Model_PlayAndPurchase_rank5_reg0.05\nEvaluating Recall@10 for Model_PlayAndPurchase_rank5_reg0.05\nEvaluation metrics logged to MLflow for Model_PlayAndPurchase_rank5_reg0.05\n\nEvaluating RMSE for Model_PlayAndPurchase_rank15_reg0.001\nEvaluating Precision@10 for Model_PlayAndPurchase_rank15_reg0.001\nEvaluating Recall@10 for Model_PlayAndPurchase_rank15_reg0.001\nEvaluation metrics logged to MLflow for Model_PlayAndPurchase_rank15_reg0.001\n\nEvaluating RMSE for Model_PlayAndPurchase_rank15_reg0.01\nEvaluating Precision@10 for Model_PlayAndPurchase_rank15_reg0.01\nEvaluating Recall@10 for Model_PlayAndPurchase_rank15_reg0.01\nEvaluation metrics logged to MLflow for Model_PlayAndPurchase_rank15_reg0.01\n\nEvaluating RMSE for Model_PlayAndPurchase_rank15_reg0.05\nEvaluating Precision@10 for Model_PlayAndPurchase_rank15_reg0.05\nEvaluating Recall@10 for Model_PlayAndPurchase_rank15_reg0.05\nEvaluation metrics logged to MLflow for Model_PlayAndPurchase_rank15_reg0.05\n\n"
     ]
    }
   ],
   "source": [
    "# Evaluate RMSE, Precision@10 and Recall@10 in all baseline and hypertuning models, stores results in a list and log metrics in MLflow\n",
    "\n",
    "# List to store all the metrics\n",
    "evaluation_results = []\n",
    "\n",
    "\n",
    "# Baseline - Play Only model evaluation\n",
    "rmse_base_play = evaluate_rmse(baseline_model_playOnly[\"model\"], baseline_model_playOnly[\"model_label\"], baseline_model_playOnly[\"predictions\"])\n",
    "precision_base_play = evaluate_precision(baseline_model_playOnly[\"model\"], test_playOnly, baseline_model_playOnly[\"model_label\"], n=10)\n",
    "recall_base_play = evaluate_recall(baseline_model_playOnly[\"model\"], test_playOnly, baseline_model_playOnly[\"model_label\"], n=10)\n",
    "# Logs the model's label and metrics to MLflow\n",
    "log_evaluation_to_mlflow(baseline_model_playOnly[\"model_label\"], rmse_base_play, precision_base_play, recall_base_play)\n",
    "# Creates a dictionary containing the model's label and metrics and appends the dictionary to evaluation_results list\n",
    "evaluation_results.append({\"model_label\": baseline_model_playOnly[\"model_label\"], \"rmse\": rmse_base_play, \"precision@10\": precision_base_play, \"recall@10\": recall_base_play})\n",
    "\n",
    "\n",
    "# Baseline Play and Purchase model evaluation\n",
    "rmse_base_playPurchase = evaluate_rmse(baseline_model_playAndPurchase[\"model\"], baseline_model_playAndPurchase[\"model_label\"], baseline_model_playAndPurchase[\"predictions\"])\n",
    "precision_base_playPurchase = evaluate_precision(baseline_model_playAndPurchase[\"model\"], test_playAndPurchase, baseline_model_playAndPurchase[\"model_label\"], n=10)\n",
    "recall_base_playPurchase = evaluate_recall(baseline_model_playAndPurchase[\"model\"], test_playAndPurchase, baseline_model_playAndPurchase[\"model_label\"], n=10)\n",
    "# Logs the model's label and metrics to MLflow\n",
    "log_evaluation_to_mlflow(baseline_model_playAndPurchase[\"model_label\"], rmse_base_playPurchase, precision_base_playPurchase, recall_base_playPurchase)\n",
    "# Creates a dictionary containing the model's label and metrics and appends the dictionary to evaluation_results list\n",
    "evaluation_results.append({\"model_label\": baseline_model_playAndPurchase[\"model_label\"], \"rmse\": rmse_base_playPurchase, \"precision@10\": precision_base_playPurchase, \"recall@10\": recall_base_playPurchase})\n",
    "\n",
    "\n",
    "# Loop through Play Only hypertuning models stored in tuned_models_playOnly to evaluate each\n",
    "for model_info in tuned_models_playOnly:\n",
    "    # Gets the model object\n",
    "    model = model_info[\"model\"]\n",
    "    # Gets the model's label   \n",
    "    model_label = model_info[\"model_label\"]\n",
    "    # Gets the model's predictions\n",
    "    predictions = model_info[\"predictions\"]\n",
    "    # Computes RMSE\n",
    "    rmse = evaluate_rmse(model, model_label, predictions)\n",
    "    # Compures precision@10\n",
    "    precision = evaluate_precision(model, test_playOnly, model_label, n=10)\n",
    "    # Computes recall@10\n",
    "    recall = evaluate_recall(model, test_playOnly, model_label, n=10)\n",
    "    # Logs model label and resulting metrics to MLflow\n",
    "    log_evaluation_to_mlflow(model_label, rmse, precision, recall)\n",
    "    # Creates a dictionary containing the model's label and metrics and appends the dictionary to evaluation_results list\n",
    "    evaluation_results.append({\"model_label\": model_label,\"rmse\": rmse,\"precision@10\": precision,\"recall@10\": recall})\n",
    "\n",
    "# Loop through Play and Purchase hypertuned models stored in tuned_models_playAndPurchase list to evaluate each\n",
    "for model_info in tuned_models_playAndPurchase:\n",
    "    # Gets the model object\n",
    "    model = model_info[\"model\"]\n",
    "    # Gets the model's label\n",
    "    model_label = model_info[\"model_label\"]\n",
    "    # Gets the model's predictions\n",
    "    predictions = model_info[\"predictions\"]\n",
    "    # Computes RMSE\n",
    "    rmse = evaluate_rmse(model, model_label, predictions)\n",
    "    # Compures precision@10\n",
    "    precision = evaluate_precision(model, test_playAndPurchase, model_label, n=10)\n",
    "    # Computes recall@10\n",
    "    recall = evaluate_recall(model, test_playAndPurchase, model_label, n=10)\n",
    "    # Logs model label and resulting metrics to MLflow\n",
    "    log_evaluation_to_mlflow(model_label, rmse, precision, recall)\n",
    "    # Creates a dictionary containing the model's label and metrics and appends the dictionary to evaluation_results list\n",
    "    evaluation_results.append({\"model_label\": model_label,\"rmse\": rmse,\"precision@10\": precision,\"recall@10\": recall})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8eb66a3-f8aa-4dba-95ae-359af0dcff75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Evaluation Metrics Ordered by Ascending RMSE\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>model_label</th><th>rmse</th><th>precision@10</th><th>recall@10</th></tr></thead><tbody><tr><td>Model_PlayAndPurchase_rank15_reg0.001</td><td>22.81787103077415</td><td>0.07418862992812196</td><td>0.315048316783646</td></tr><tr><td>Model_PlayAndPurchase_rank5_reg0.001</td><td>22.819261244925393</td><td>0.06532345894140741</td><td>0.26423294531729397</td></tr><tr><td>Model_PlayAndPurchase_rank15_reg0.01</td><td>22.822413126887025</td><td>0.07390546721847263</td><td>0.31510005339821046</td></tr><tr><td>Model_PlayAndPurchase_rank15_reg0.05</td><td>22.82861544937509</td><td>0.07251143541712193</td><td>0.3104944818374553</td></tr><tr><td>Model_PlayAndPurchase</td><td>22.830322090709018</td><td>0.0711609671095635</td><td>0.3024937037572767</td></tr><tr><td>Model_PlayAndPurchase_rank5_reg0.01</td><td>22.830924360370908</td><td>0.0642343715966022</td><td>0.26130908063285235</td></tr><tr><td>Model_PlayAndPurchase_rank5_reg0.05</td><td>22.836230134906145</td><td>0.06364626443040738</td><td>0.25978486007266216</td></tr><tr><td>Model_PlayOnly_rank5_reg0.001</td><td>45.09940210765486</td><td>0.05382155420530737</td><td>0.242081473162181</td></tr><tr><td>Model_PlayOnly_rank5_reg0.01</td><td>45.10322397203067</td><td>0.053405820275022754</td><td>0.2387107795256216</td></tr><tr><td>Model_PlayOnly_rank5_reg0.05</td><td>45.10899115281792</td><td>0.05308602494403462</td><td>0.23882793384400106</td></tr><tr><td>Model_PlayOnly</td><td>45.12536208580551</td><td>0.05356571794051682</td><td>0.23270451239058743</td></tr><tr><td>Model_PlayOnly_rank15_reg0.01</td><td>45.13058603878034</td><td>0.054940837863765864</td><td>0.24778554839566183</td></tr><tr><td>Model_PlayOnly_rank15_reg0.001</td><td>45.13069415935404</td><td>0.05519667412855641</td><td>0.24818893959596358</td></tr><tr><td>Model_PlayOnly_rank15_reg0.05</td><td>45.13431802607922</td><td>0.05404541093699903</td><td>0.2466978850874764</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Model_PlayAndPurchase_rank15_reg0.001",
         22.81787103077415,
         0.07418862992812196,
         0.315048316783646
        ],
        [
         "Model_PlayAndPurchase_rank5_reg0.001",
         22.819261244925393,
         0.06532345894140741,
         0.26423294531729397
        ],
        [
         "Model_PlayAndPurchase_rank15_reg0.01",
         22.822413126887025,
         0.07390546721847263,
         0.31510005339821046
        ],
        [
         "Model_PlayAndPurchase_rank15_reg0.05",
         22.82861544937509,
         0.07251143541712193,
         0.3104944818374553
        ],
        [
         "Model_PlayAndPurchase",
         22.830322090709018,
         0.0711609671095635,
         0.3024937037572767
        ],
        [
         "Model_PlayAndPurchase_rank5_reg0.01",
         22.830924360370908,
         0.0642343715966022,
         0.26130908063285235
        ],
        [
         "Model_PlayAndPurchase_rank5_reg0.05",
         22.836230134906145,
         0.06364626443040738,
         0.25978486007266216
        ],
        [
         "Model_PlayOnly_rank5_reg0.001",
         45.09940210765486,
         0.05382155420530737,
         0.242081473162181
        ],
        [
         "Model_PlayOnly_rank5_reg0.01",
         45.10322397203067,
         0.053405820275022754,
         0.2387107795256216
        ],
        [
         "Model_PlayOnly_rank5_reg0.05",
         45.10899115281792,
         0.05308602494403462,
         0.23882793384400106
        ],
        [
         "Model_PlayOnly",
         45.12536208580551,
         0.05356571794051682,
         0.23270451239058743
        ],
        [
         "Model_PlayOnly_rank15_reg0.01",
         45.13058603878034,
         0.054940837863765864,
         0.24778554839566183
        ],
        [
         "Model_PlayOnly_rank15_reg0.001",
         45.13069415935404,
         0.05519667412855641,
         0.24818893959596358
        ],
        [
         "Model_PlayOnly_rank15_reg0.05",
         45.13431802607922,
         0.05404541093699903,
         0.2466978850874764
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "model_label",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rmse",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "precision@10",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "recall@10",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display Evaluation Summary\n",
    "\n",
    "# Convert list of metrics to Spark DataFrame\n",
    "evaluation_df = spark.createDataFrame([Row(**row) for row in evaluation_results])\n",
    "\n",
    "# Display the summary table ordered by RMSE\n",
    "print(\"Summary of Evaluation Metrics Ordered by Ascending RMSE\")\n",
    "evaluation_df.orderBy(\"rmse\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1401c41-4906-483f-ab03-4245cdb3f833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Precision Model: Model_PlayAndPurchase_rank15_reg0.001\nBest Recall Model: Model_PlayAndPurchase_rank15_reg0.01\n"
     ]
    }
   ],
   "source": [
    "# Select best models by Precision@10 and Recall@10\n",
    "\n",
    "# Get best model by Precision@10 in the evaluation_df DataFrame\n",
    "# Orders the DataFrame by precision@10 descending order and gets the first row\n",
    "best_precision_row = evaluation_df.orderBy(col(\"precision@10\").desc()).first()\n",
    "# Get best model by Recall@10 in the evaluation_df DataFrame\n",
    "# Orders the DataFrame by recall@10 descending order and gets the first row\n",
    "best_recall_row = evaluation_df.orderBy(col(\"recall@10\").desc()).first()\n",
    "\n",
    "# Extract model labels\n",
    "# Extract the model_label value from the row in the DataFrame with the best Precision@10 \n",
    "best_precision_label = best_precision_row[\"model_label\"]\n",
    "# Extract the model_label value from the row in the DataFrame with the best Recall@10 \n",
    "best_recall_label = best_recall_row[\"model_label\"]\n",
    "\n",
    "# Gather all models into one list\n",
    "# This includes the two baseline models and all models trained with hyperparameter tuning\n",
    "# All models were created by passing the train/test sets to train_baseline_model and train_als_with_gridsearch\n",
    "all_models = [baseline_model_playOnly, baseline_model_playAndPurchase] + tuned_models_playOnly + tuned_models_playAndPurchase\n",
    "\n",
    "# Find in the all_models list, the model dictionary that matches the best_precision_label\n",
    "# next() gives the first match found in the list\n",
    "best_precision_model_info = next(m for m in all_models if m[\"model_label\"] == best_precision_label)\n",
    "# Find in the all_models list, the model dictionary that matches the best_recall_label\n",
    "# next() gives the first match found in the list\n",
    "best_recall_model_info = next(m for m in all_models if m[\"model_label\"] == best_recall_label)\n",
    "\n",
    "# Get the model object from the dictionary that matches the best_precission_label\n",
    "best_precision_model = best_precision_model_info[\"model\"]\n",
    "# Get the model object from the dictionary that matches the best_recall_label\n",
    "best_recall_model = best_recall_model_info[\"model\"]\n",
    "\n",
    "# Display best models' labels\n",
    "print(f\"Best Precision Model: {best_precision_label}\")\n",
    "print(f\"Best Recall Model: {best_recall_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08a2ace3-35ab-4805-9f58-ba2cfc581fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The evaluation of all ALS models using RMSE, Precision@10, and Recall@10 confirmed that the Play and Purchase variants consistently outperformed the Play Only models across all metrics.\n",
    "\n",
    "Similarly, models with a higher rank value tended to perform better in terms of RMSE, likely due to the increase in the dimensionality of vectors, enabling them to capture more nuances and hidden patterns in the data.\n",
    "\n",
    "The best performing model in terms of Precision@10 was Model_PlayAndPurchase_rank15_reg0.001, showing a RMSE of 22.82, Precision@10 of 0.074, and Recall@10 of 0.315\n",
    "\n",
    "The best performing model in terms of Recall@10 was Model_PlayAndPurchase_rank15_reg0.01, showing a RMSE of 22.82, Precision@10 of 0.074, and Recall@10 of 0.315\n",
    "\n",
    "These models were able to retrieve over 31% of the games  the members actually interacted with, using only 10 recommendations, and mantained an average of approximately 0.74 relevant items in the top-10 recommendation list. These results reflect good performance considering the nature of the data and the lack of explicit ratings.\n",
    "\n",
    "The Play Only models showed significantly higher RMSE but lower values for both Precision and Recall, which indicates the added value of incorporating purchase behaviour into the rating to improve both accuracy and relevance when generating recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a435c78-9ec7-44db-a755-1f73622858d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea9fff1f-0322-484e-b32f-5cac132c5d67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59733519-85f2-4d75-9c86-65c532f21e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7. Generating Recommendations##\n",
    "\n",
    "In the final stage, we focused on demonstrating the use of the best-performing models to generate game recommendations. Given that models can only generate recommendations for members who have been included in their training sets, we developed a cold start fallback function, `generate_fallback_recommendations(n=10)`  that returns the top-n most played games by aggregating the total playtime across all users for each game using the pivoted dataset from the pre-processing stage and maps each gameIndex back to its original gameName using the fitted StringIndexer, so that members who were not seen by a model during its training can still receive recommendations.\n",
    "\n",
    "Then, we used the `recommendForAllUsers`  Spark method to generate a DataFrame of Top 10 recommendations for all users using the ALS models with the highest Precision and Recall. The generated DataFrames are stored in the top10_precision and top10_recall variables. If the same model achieved the best result in the two metrics, recommendations were produced using only that model. Otherwise, two separate recommendation DataFrames were created using both models to allow for comparison.\n",
    "\n",
    "To retrieve personalised recommendations for a given user, we implemented the  `generate_one_user_recommendations()` function. This function takes a DataFrame with the ten best recommendations for all users generated by `recommendForAllUsers` Spark method, the model's label and the original member ID (string) as input and maps the original memberID to its corresponding memberIndex using the indexed DataFrame. If the user is not found or was not seen during model training, it calls the cold start fallback  `generate_fallback_recommendations(n=10)` function to return the most played games. If the member ID is not a new member and it was part of the model's training set, the function filters the recommendation DataFrame for that specific user, explodes the nested recommendation list into individual game entries, and joins with the mapping DataFrame to retrieve the game names alongside predicted ratings. Consequently, we applied this function to the first user in the dataset and displayed the resulting top-10 recommendations, showing both predicted ratings and game names.\n",
    "\n",
    " To contextualise the model’s outputs, we also called the fallback function to display the 10 most played games across the entire dataset, allowing for a side-by-side comparison with personalised results.\n",
    "\n",
    "This stage confirms that the trained ALS models can produce individualised, data-driven recommendations for users while also supporting fallback options for new or unseen members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805d309a-686a-4470-84f5-53a2e3975719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cold start function\n",
    "# Function to genereate recommendations using the 10 most played games accross all users\n",
    "# Use as cold start function to generate recommedations for members not in the original dataset (new members) \n",
    "# Also, use for members not on the training set in a trained model (ALS can not generate recommendations for users not seen during training) \n",
    "def generate_fallback_recommendations(n=10):\n",
    "\n",
    "    # Groups the dataset by gameIndex to aggregate total play time across all users\n",
    "    top_games_df = (pivotedDF.groupBy(\"gameIndex\").agg(spark_sum(\"played_hours\").alias(\"totalHours\")).orderBy(col(\"totalHours\"). desc()).limit(n))\n",
    "\n",
    "    # Retrieves the game names from the StringIndexer transformed SteamDF DataFrame \n",
    "    game_labels = game_indexer_model.labels\n",
    "\n",
    "    # Creates a mapping DataFrame to convert the numeric gameIndex back to game names\n",
    "    game_mapping_df = spark.createDataFrame([Row(gameIndex=i, gameName=name) for i, name in enumerate(game_labels)])\n",
    "\n",
    "    # Joins the aggregated top games with their names for display purposes\n",
    "    top_games_named = top_games_df.join(game_mapping_df, on=\"gameIndex\", how=\"left\").withColumn(\"totalHours\", spark_round(col(\"totalHours\"), 2))\n",
    "\n",
    "    # Returns a DataFrame of the top n most played games with their names and total hours.\n",
    "    return top_games_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ec8639-9443-4385-9b07-4ae7c38da648",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nRecommendations with the Best Precision Model: Model_PlayAndPurchase_rank15_reg0.001\nRecommendations with the Best Recall Model: Model_PlayAndPurchase_rank15_reg0.01\n"
     ]
    }
   ],
   "source": [
    "# Generate a DataFrame with 10 recommendations for all users in the training set\n",
    "# If the same model achieved the best precision and the best recall, it uses this model, otherwise uses both models and generates two sets of recommendaions for comparison\n",
    "# It returns Spark DataFrame(s) where each row corresponds to a user, and the recommendations column contains a nested list of recommended items and predicted ratings\n",
    "if best_precision_label == best_recall_label:\n",
    "    print(f\"\\nSame model achieved both best Precision@10 and Recall@10: {best_precision_label}\")\n",
    "    # Generates 10 recommendations using the model that achieved best precission\n",
    "    top10_precision = best_precision_model.recommendForAllUsers(10)\n",
    "else:\n",
    "    print(f\"\\nRecommendations with the Best Precision Model: {best_precision_label}\")\n",
    "    top10_precision = best_precision_model.recommendForAllUsers(10)\n",
    "    print(f\"Recommendations with the Best Recall Model: {best_recall_label}\")\n",
    "    top10_recall = best_recall_model.recommendForAllUsers(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a41b3f4-267a-4ba6-94f1-c0929d115d77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to get 10 recommendations for one user from a recommendation DataFrame\n",
    "def generate_one_user_recommendations(recommendation_df, member_id, model_label):\n",
    "    \n",
    "    # Use the DataFrame transformed with the StringIndexer to find the row where the corresponding numeric memberIndex for the member_id string passed as argument is\n",
    "    member_index_row = pivotedDF.select(\"memberID\", \"memberIndex\").filter(col(\"memberID\") == member_id).first()\n",
    "\n",
    "    \n",
    "    # If the member_id is not found in the StringIndexer transformed SteamDF DataFrame return the fallback ten most played games (cold start)\n",
    "    # If member_index_row is assigned a null value it means the member_id is wrong or a new member not in the original raw data\n",
    "    if not member_index_row:\n",
    "        print(f\"\\nMember ID {member_id} not found in pivotedDF. Returning fallback recommendations.\")\n",
    "        # Uses the custom function to generate fallback recommendations\n",
    "        fallback_df = generate_fallback_recommendations()\n",
    "        # Display the fallback recommendations\n",
    "        print(f\"\\nTop 10 Popular Games (Fallback for Member ID {member_id}):\")\n",
    "        fallback_df.select(\"gameIndex\", \"gameName\", \"totalHours\").show()\n",
    "        # Returns the fallback recommendations\n",
    "        return fallback_df\n",
    "    else:\n",
    "        # If the member_id is found in the StringIndexer transformed SteamDF DataFrame, extracts the corresponding numberIndex\n",
    "        member_index = member_index_row[\"memberIndex\"]\n",
    "\n",
    "\n",
    "\n",
    "    # Check if the member was not seen during the model's training\n",
    "    # Assumes that the recommendation_df DataFrame would containg recommendations for the member_id member if it has been part of the model's trainig data\n",
    "    if recommendation_df.filter(col(\"memberIndex\") == member_index).count() == 0:\n",
    "        print(f\"\\nCold-start: Member ID {member_id} was not seen during training.\")\n",
    "        # Generate fallback recommendations using the custom function if the recommendations do not included the member_id\n",
    "        fallback_df = generate_fallback_recommendations()\n",
    "        # Display the fallback recommendations\n",
    "        print(f\"\\nTop 10 Popular Games (Fallback for Member ID {member_id}):\")\n",
    "        fallback_df.select(\"gameIndex\", \"gameName\", \"totalHours\").show()\n",
    "        # Return the fallback recommendations\n",
    "        return fallback_df      \n",
    "\n",
    "    # Continue if member was seen by the model during training and it is not a new member\n",
    "\n",
    "    # Filter just the recommendations for the specific user using their memberIndex\n",
    "    user_recs = recommendation_df.filter(col(\"memberIndex\") == member_index)\n",
    "\n",
    "    # Flatten the nested recommendations list (which contains gameIndex and predicted_rating)\n",
    "    # This gives one row per recommended game for the user\n",
    "    user_recs_flat = user_recs.withColumn(\"rec\", explode(\"recommendations\")).select(\n",
    "        \"memberIndex\",\n",
    "        col(\"rec.gameIndex\").alias(\"gameIndex\"),\n",
    "        col(\"rec.rating\").alias(\"predicted_rating\")\n",
    "    )\n",
    "\n",
    "    # Get the list of game names used in the StringIndexer \n",
    "    game_labels = game_indexer_model.labels\n",
    "\n",
    "    # Convert the list of games into a DataFrame\n",
    "    game_mapping_df = spark.createDataFrame([Row(gameIndex=i, gameName=name) for i, name in enumerate(game_labels)])\n",
    "\n",
    "    # Join the flattened recommendations with the game names\n",
    "    user_recs_named = user_recs_flat.join(game_mapping_df, on=\"gameIndex\", how=\"left\").withColumn(\"predicted_rating\", spark_round(col(\"predicted_rating\"), 2))\n",
    "\n",
    "\n",
    "    # Show the top-10 recommended games with predicted ratings for that user\n",
    "    label_info = f\" ({model_label})\" if model_label else \"\"\n",
    "    print(f\"\\nTop 10 Recommendations for Member ID {member_id}{label_info}:\")\n",
    "    user_recs_named.select(\"gameIndex\", \"gameName\", \"predicted_rating\").show(truncate=False)\n",
    "\n",
    "    # Returns the 10 recommendations\n",
    "    return user_recs_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98db6c8f-b4c9-45fa-a040-6cbe24538c90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nGenerating recommendations for best Precision model: Model_PlayAndPurchase_rank15_reg0.001\n\nTop 10 Recommendations for Member ID 151603712 (Model_PlayAndPurchase_rank15_reg0.001):\n+---------+-------------------------------+----------------+\n|gameIndex|gameName                       |predicted_rating|\n+---------+-------------------------------+----------------+\n|2        |Unturned                       |1.45            |\n|11       |The Elder Scrolls V Skyrim     |1.0             |\n|3        |Counter-Strike Global Offensive|1.0             |\n|6        |Left 4 Dead 2                  |0.95            |\n|17       |Sid Meier's Civilization V     |0.93            |\n|24       |Terraria                       |0.87            |\n|26       |Borderlands 2                  |0.86            |\n|8        |Warframe                       |0.86            |\n|0        |Dota 2                         |0.85            |\n|135      |Fallout 4                      |0.81            |\n+---------+-------------------------------+----------------+\n\n\nGenerating recommendations for best Recall model: Model_PlayAndPurchase_rank15_reg0.01\n\nTop 10 Recommendations for Member ID 151603712 (Model_PlayAndPurchase_rank15_reg0.01):\n+---------+-------------------------------+----------------+\n|gameIndex|gameName                       |predicted_rating|\n+---------+-------------------------------+----------------+\n|2        |Unturned                       |1.39            |\n|11       |The Elder Scrolls V Skyrim     |0.99            |\n|3        |Counter-Strike Global Offensive|0.94            |\n|6        |Left 4 Dead 2                  |0.93            |\n|17       |Sid Meier's Civilization V     |0.92            |\n|26       |Borderlands 2                  |0.91            |\n|0        |Dota 2                         |0.86            |\n|8        |Warframe                       |0.82            |\n|24       |Terraria                       |0.79            |\n|12       |Robocraft                      |0.78            |\n+---------+-------------------------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Get the first member ID from the dataset (original string ID before indexing)\n",
    "first_member_id = steamDF.select(\"memberID\").first()[\"memberID\"]\n",
    "\n",
    "# Only generate one set of recommendations if both metrics picked the same model\n",
    "if best_precision_label == best_recall_label:\n",
    "    print(f\"\\nGenerating recommendations for best model (Precision & Recall): {best_precision_label}\")\n",
    "    # Generate 10 recommendations for the first member using the model with best precision\n",
    "    generate_one_user_recommendations(top10_precision, first_member_id, best_precision_label)\n",
    "else:\n",
    "    print(f\"\\nGenerating recommendations for best Precision model: {best_precision_label}\")\n",
    "    # Generate 10 recommendations for the first member using the model with best precision\n",
    "    generate_one_user_recommendations(top10_precision, first_member_id, best_precision_label)\n",
    "\n",
    "    print(f\"\\nGenerating recommendations for best Recall model: {best_recall_label}\")\n",
    "    # Generate 10 recommendations for the first member using the model with best recall\n",
    "    generate_one_user_recommendations(top10_recall, first_member_id, best_recall_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c5af11e-eefc-48a3-ab78-a5a2792ff1ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------------------------+----------+\n|gameIndex|gameName                                   |totalHours|\n+---------+-------------------------------------------+----------+\n|0        |Dota 2                                     |981684.6  |\n|3        |Counter-Strike Global Offensive            |322771.6  |\n|1        |Team Fortress 2                            |173673.3  |\n|7        |Counter-Strike                             |134261.1  |\n|17       |Sid Meier's Civilization V                 |99821.3   |\n|5        |Counter-Strike Source                      |96075.5   |\n|11       |The Elder Scrolls V Skyrim                 |70889.3   |\n|10       |Garry's Mod                                |49725.3   |\n|45       |Call of Duty Modern Warfare 2 - Multiplayer|42009.9   |\n|6        |Left 4 Dead 2                              |33596.7   |\n+---------+-------------------------------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Call the cold start function to compare the 10 games with most hours played accross all users with the 10 games recommended to the user in the output above\n",
    "fallback_df = generate_fallback_recommendations(10)\n",
    "\n",
    "# Show the fallback recommendations \n",
    "fallback_df.select(\"gameIndex\", \"gameName\", \"totalHours\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b5a8d6e-9972-4c44-abfd-c1f2c14749c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output confirmed that the ALS models trained earlier were able to generate personalised top-10 recommendations for each member seen during training and personalised top-10 recommendations for a single member, each including a predicted rating, with higher values representing stronger indications of the member's likely preference for the game. \n",
    "\n",
    " Given the implicit feedback nature of the training data, the predicted rating shown for each game in the top-10 recommendations does not represent an estimate of how many hours the member will play the game or the probability that the member would buy the game. Instead, the rating indicates how confident the model is about the strength of the member’s potential interest in the game. Therefore, these values should be interpreted in relative terms considering the ratings of the other games in the list of recommendations.\n",
    "\n",
    "The top-10 lists generated by the models with the best Precision (Model_PlayAndPurchase_rank15_reg0.001) and the best Recall (Model_PlayAndPurchase_rank15_reg0.01) had some overlap, with games like Unturned, Skyrim, and Counter-Strike: Global Offensive appearing in both, suggesting that both models agreed on some of the most relevant games.\n",
    "\n",
    "However, there were also a few differences between the two lists, such as Robocraft appearing in the best recall model’s list but not in the precision one, showing how the two models prioritise slightly different things with the precision model focusing more on returning highly relevant games, while the recall model aiming to include more of the games the user interacted with overall, even if their predicted ratings were slightly lower.\n",
    "\n",
    "Lastly, the fallback recommendations also worked as expected, returning the ten most played games across all users some of which also appeared in the personalised lists but most were unique to the fallback list showing how personalised recommendations can go beyond the popularity of the games to recommend games based on individual preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb909a9c-98b4-4b44-bcc9-6f25a33ab3fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "886e1876-fb91-4ceb-9bdb-36974691a51a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89efd2bb-33f4-4512-b5b4-cfb5c6296092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##8. Conclusion##\n",
    "\n",
    "In this notebook, we implemented a collaborative filtering recommender system using matrix factorization via the ALS algorithm in Spark MLlib, to train models entirely on implicit feedback from user-game interactions, using different value combinations of rank (dimensionality of latent factors vectors) and regularisation regParams for model hyperparameter tuning and programmatically tracking models with MLflow. Two approaches were tested, one using hours played only, and another combining hours played with a weighted purchase flag as implicit feedback.\n",
    "\n",
    "The system follows a collaborative filtering approach by learning patterns from user/game interactions only, without needing game metadata such as genre or release year or relying on explicit user ratings.  \n",
    "\n",
    "The model trained on both play and purchase behaviours consistently outperformed the play-only model. The best model in terms of Precision@10 was Model_PlayAndPurchase_rank15_reg0.001 (Precision@10: 0.074). The best Recall@10 was achieved by Model_PlayAndPurchase_rank15_reg0.01 (Recall@10: 0.315).\n",
    "\n",
    "Companies using this system could improve customer engagement by helping their users discover games they are more likely to enjoy and potentially increase time spent on the platform while generating more sales or playtime. The choice between precision and recall depends on the business goal. If their priority is offering highly relevant games at the top of the list, the model with the best precision should be chosen. If the goal is to maximise the exposure to relevant games, recall becomes more important.\n",
    "\n",
    "Some of the limitations of the system include the cold start problem due to the inherent impossibility of ALS models to generate personalised recommendations for members that were not included in the models' training sets in addition to the reduced explainability of the predicted ratings in the recommendations due to the nature of the implicit feedback approached used.\n",
    "\n",
    "Future work could include functionality improvements such as filtering out games already owned or played by a member before generating recommendations. It could also seek model performance improvement by using more implicit feedback elements such as clickstream and time since the last interaction, expanding the quantity and range of hyperparameters used during hyperparameter tuning and including techniques such as early stopping, trying different weights for the purchase feedback, or exploring hybrid models combining collaborative and content-based filtering for better cold start handling and more personalised recommendations. Moreover, frontier research could be explored to carry out posthoc analysis aiming to interpret what the models have captured and patterns in their latent features that may suggest correlations with the implicit feedback elements the model was trained on such as patterns in playtime and purchase behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c37a5e40-d2df-49e5-9c6d-2dde5c92c892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19ec7542-550f-4720-8575-f8e6c76f71b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Appendix 1: MLflow Logs##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a20c7a94-d187-46d1-9008-fc3246e13a7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLflow Experiment ID: 1210934385966680\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>run_id</th><th>model</th><th>rank</th><th>regParam</th><th>maxIter</th><th>implicitPrefs</th><th>rmse</th><th>precision</th><th>recall</th></tr></thead><tbody><tr><td>d88273e7315a49758eb8e8ff092d610c</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>311c01517f144e1d9d5d5e071a287cff</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>782dc91def454e05951215e67cc92a9d</td><td>Model_PlayOnly</td><td>10</td><td>0.1</td><td>10</td><td>True</td><td>null</td><td>null</td><td>null</td></tr><tr><td>d642568531a14e07bab145abb7b81bfa</td><td>Model_PlayAndPurchase</td><td>10</td><td>0.1</td><td>10</td><td>True</td><td>null</td><td>null</td><td>null</td></tr><tr><td>b50c8d0ec57f42dba16d1dc96619a625</td><td>Model_PlayOnly</td><td>5</td><td>0.001</td><td>10</td><td>True</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0698c3f2589745f190894486e1fba0ca</td><td>Model_PlayOnly</td><td>5</td><td>0.01</td><td>10</td><td>True</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1d8a0338693a4dcea2feaf42abf110a2</td><td>Model_PlayOnly</td><td>5</td><td>0.05</td><td>10</td><td>True</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2bec427e85f147e9affe164fd117d06f</td><td>Model_PlayOnly</td><td>15</td><td>0.001</td><td>10</td><td>True</td><td>null</td><td>null</td><td>null</td></tr><tr><td>f36f0699eebd4667a9f9510b3a7494e0</td><td>Model_PlayOnly</td><td>15</td><td>0.01</td><td>10</td><td>True</td><td>null</td><td>null</td><td>null</td></tr><tr><td>6c2603a4dc8d4ef6a90c88d09646f2ae</td><td>Model_PlayOnly</td><td>15</td><td>0.05</td><td>10</td><td>True</td><td>null</td><td>null</td><td>null</td></tr><tr><td>b175cf20f20d440fb6788b66d48c3c42</td><td>Model_PlayAndPurchase</td><td>5</td><td>0.001</td><td>10</td><td>True</td><td>null</td><td>null</td><td>null</td></tr><tr><td>864262b939b147e8ad578eb5e04409b2</td><td>Model_PlayAndPurchase</td><td>5</td><td>0.01</td><td>10</td><td>True</td><td>null</td><td>null</td><td>null</td></tr><tr><td>b7318fe8f0a84848ae06a8a9e636f695</td><td>Model_PlayAndPurchase</td><td>5</td><td>0.05</td><td>10</td><td>True</td><td>null</td><td>null</td><td>null</td></tr><tr><td>ac74f35b046a48a6aaeb27e7ae543de6</td><td>Model_PlayAndPurchase</td><td>15</td><td>0.001</td><td>10</td><td>True</td><td>null</td><td>null</td><td>null</td></tr><tr><td>d91b40de1c9d4489acb5a831ed11297b</td><td>Model_PlayAndPurchase</td><td>15</td><td>0.01</td><td>10</td><td>True</td><td>null</td><td>null</td><td>null</td></tr><tr><td>c70d82c29c26422b8d4d02e249c18775</td><td>Model_PlayAndPurchase</td><td>15</td><td>0.05</td><td>10</td><td>True</td><td>null</td><td>null</td><td>null</td></tr><tr><td>cce8732656b641768a13220a3a6321d2</td><td>Model_PlayOnly</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.05356572</td><td>0.2327045</td></tr><tr><td>80017f02d0884d32ab8dff5e9e87b134</td><td>Model_PlayAndPurchase</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.071160965</td><td>0.3024937</td></tr><tr><td>210d35ae4cfe417d8fc90f938a86656b</td><td>Model_PlayOnly_rank5_reg0.001</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.053821553</td><td>0.24208148</td></tr><tr><td>075fc6ef8bfe458891d533fb3b52c898</td><td>Model_PlayOnly_rank5_reg0.01</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.05340582</td><td>0.23871078</td></tr><tr><td>ff57d5a0cb234e32822d103f784078cf</td><td>Model_PlayOnly_rank5_reg0.05</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.053086024</td><td>0.23882793</td></tr><tr><td>fffc8806778644b2a98f34240895f359</td><td>Model_PlayOnly_rank15_reg0.001</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.055196673</td><td>0.24818894</td></tr><tr><td>86a3c5e2b32a4757a50414d204c7fa87</td><td>Model_PlayOnly_rank15_reg0.01</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.05494084</td><td>0.24778555</td></tr><tr><td>bfc40d995c014113ac697bb77ad618b3</td><td>Model_PlayOnly_rank15_reg0.05</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.054045413</td><td>0.24669789</td></tr><tr><td>06b5a4500bd044d4b1df9702ce4fe458</td><td>Model_PlayAndPurchase_rank5_reg0.001</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.06532346</td><td>0.26423293</td></tr><tr><td>7108b3794f744d918c2a38f74f6bc0e5</td><td>Model_PlayAndPurchase_rank5_reg0.01</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.06423437</td><td>0.2613091</td></tr><tr><td>df554e8cc1be46cdbd54ef713e235d48</td><td>Model_PlayAndPurchase_rank5_reg0.05</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.063646264</td><td>0.25978485</td></tr><tr><td>f557e8c71b094e73a4823d7681c7e026</td><td>Model_PlayAndPurchase_rank15_reg0.001</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.07418863</td><td>0.3150483</td></tr><tr><td>760b560690044e83964a4528ac6f2ab8</td><td>Model_PlayAndPurchase_rank15_reg0.01</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.07390547</td><td>0.31510004</td></tr><tr><td>09f6a55d995348dcbde80778f495f034</td><td>Model_PlayAndPurchase_rank15_reg0.05</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.072511435</td><td>0.31049448</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "d88273e7315a49758eb8e8ff092d610c",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "311c01517f144e1d9d5d5e071a287cff",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "782dc91def454e05951215e67cc92a9d",
         "Model_PlayOnly",
         "10",
         "0.1",
         "10",
         "True",
         null,
         null,
         null
        ],
        [
         "d642568531a14e07bab145abb7b81bfa",
         "Model_PlayAndPurchase",
         "10",
         "0.1",
         "10",
         "True",
         null,
         null,
         null
        ],
        [
         "b50c8d0ec57f42dba16d1dc96619a625",
         "Model_PlayOnly",
         "5",
         "0.001",
         "10",
         "True",
         null,
         null,
         null
        ],
        [
         "0698c3f2589745f190894486e1fba0ca",
         "Model_PlayOnly",
         "5",
         "0.01",
         "10",
         "True",
         null,
         null,
         null
        ],
        [
         "1d8a0338693a4dcea2feaf42abf110a2",
         "Model_PlayOnly",
         "5",
         "0.05",
         "10",
         "True",
         null,
         null,
         null
        ],
        [
         "2bec427e85f147e9affe164fd117d06f",
         "Model_PlayOnly",
         "15",
         "0.001",
         "10",
         "True",
         null,
         null,
         null
        ],
        [
         "f36f0699eebd4667a9f9510b3a7494e0",
         "Model_PlayOnly",
         "15",
         "0.01",
         "10",
         "True",
         null,
         null,
         null
        ],
        [
         "6c2603a4dc8d4ef6a90c88d09646f2ae",
         "Model_PlayOnly",
         "15",
         "0.05",
         "10",
         "True",
         null,
         null,
         null
        ],
        [
         "b175cf20f20d440fb6788b66d48c3c42",
         "Model_PlayAndPurchase",
         "5",
         "0.001",
         "10",
         "True",
         null,
         null,
         null
        ],
        [
         "864262b939b147e8ad578eb5e04409b2",
         "Model_PlayAndPurchase",
         "5",
         "0.01",
         "10",
         "True",
         null,
         null,
         null
        ],
        [
         "b7318fe8f0a84848ae06a8a9e636f695",
         "Model_PlayAndPurchase",
         "5",
         "0.05",
         "10",
         "True",
         null,
         null,
         null
        ],
        [
         "ac74f35b046a48a6aaeb27e7ae543de6",
         "Model_PlayAndPurchase",
         "15",
         "0.001",
         "10",
         "True",
         null,
         null,
         null
        ],
        [
         "d91b40de1c9d4489acb5a831ed11297b",
         "Model_PlayAndPurchase",
         "15",
         "0.01",
         "10",
         "True",
         null,
         null,
         null
        ],
        [
         "c70d82c29c26422b8d4d02e249c18775",
         "Model_PlayAndPurchase",
         "15",
         "0.05",
         "10",
         "True",
         null,
         null,
         null
        ],
        [
         "cce8732656b641768a13220a3a6321d2",
         "Model_PlayOnly",
         null,
         null,
         null,
         null,
         null,
         0.05356572,
         0.2327045
        ],
        [
         "80017f02d0884d32ab8dff5e9e87b134",
         "Model_PlayAndPurchase",
         null,
         null,
         null,
         null,
         null,
         0.071160965,
         0.3024937
        ],
        [
         "210d35ae4cfe417d8fc90f938a86656b",
         "Model_PlayOnly_rank5_reg0.001",
         null,
         null,
         null,
         null,
         null,
         0.053821553,
         0.24208148
        ],
        [
         "075fc6ef8bfe458891d533fb3b52c898",
         "Model_PlayOnly_rank5_reg0.01",
         null,
         null,
         null,
         null,
         null,
         0.05340582,
         0.23871078
        ],
        [
         "ff57d5a0cb234e32822d103f784078cf",
         "Model_PlayOnly_rank5_reg0.05",
         null,
         null,
         null,
         null,
         null,
         0.053086024,
         0.23882793
        ],
        [
         "fffc8806778644b2a98f34240895f359",
         "Model_PlayOnly_rank15_reg0.001",
         null,
         null,
         null,
         null,
         null,
         0.055196673,
         0.24818894
        ],
        [
         "86a3c5e2b32a4757a50414d204c7fa87",
         "Model_PlayOnly_rank15_reg0.01",
         null,
         null,
         null,
         null,
         null,
         0.05494084,
         0.24778555
        ],
        [
         "bfc40d995c014113ac697bb77ad618b3",
         "Model_PlayOnly_rank15_reg0.05",
         null,
         null,
         null,
         null,
         null,
         0.054045413,
         0.24669789
        ],
        [
         "06b5a4500bd044d4b1df9702ce4fe458",
         "Model_PlayAndPurchase_rank5_reg0.001",
         null,
         null,
         null,
         null,
         null,
         0.06532346,
         0.26423293
        ],
        [
         "7108b3794f744d918c2a38f74f6bc0e5",
         "Model_PlayAndPurchase_rank5_reg0.01",
         null,
         null,
         null,
         null,
         null,
         0.06423437,
         0.2613091
        ],
        [
         "df554e8cc1be46cdbd54ef713e235d48",
         "Model_PlayAndPurchase_rank5_reg0.05",
         null,
         null,
         null,
         null,
         null,
         0.063646264,
         0.25978485
        ],
        [
         "f557e8c71b094e73a4823d7681c7e026",
         "Model_PlayAndPurchase_rank15_reg0.001",
         null,
         null,
         null,
         null,
         null,
         0.07418863,
         0.3150483
        ],
        [
         "760b560690044e83964a4528ac6f2ab8",
         "Model_PlayAndPurchase_rank15_reg0.01",
         null,
         null,
         null,
         null,
         null,
         0.07390547,
         0.31510004
        ],
        [
         "09f6a55d995348dcbde80778f495f034",
         "Model_PlayAndPurchase_rank15_reg0.05",
         null,
         null,
         null,
         null,
         null,
         0.072511435,
         0.31049448
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "run_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "model",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rank",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "regParam",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "maxIter",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "implicitPrefs",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rmse",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "precision",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "recall",
         "type": "\"float\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the MLflow logs\n",
    "# Allows review to the experiment logs without direct MLflow server access.\n",
    "\n",
    "# Initialise MLflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "# Get the experiment\n",
    "experiment = client.get_experiment_by_name(experiment_path)\n",
    "\n",
    "# \n",
    "if experiment is None:\n",
    "    print(f\"Experiment not found at path: {experiment_path}\")\n",
    "else:\n",
    "    print(\"Using MLflow Experiment ID:\", experiment.experiment_id)\n",
    "\n",
    "    # Get all runs in ascending start time\n",
    "    runs = client.search_runs(experiment.experiment_id, order_by=[\"start_time ASC\"])\n",
    "\n",
    "    # Format MLflow runs as Spark DataFrame rows\n",
    "    rows = []\n",
    "    for run in runs:\n",
    "        data = run.data\n",
    "\n",
    "        '''\n",
    "        rows.append({\n",
    "            \"run_id\": run.info.run_id,\n",
    "            \"model\": data.params.get(\"model\", \"N/A\"),\n",
    "            \"rank\": data.params.get(\"rank\", \"N/A\"),\n",
    "            \"regParam\": data.params.get(\"regParam\", \"N/A\"),\n",
    "            \"maxIter\": data.params.get(\"maxIter\", \"N/A\"),\n",
    "            \"implicitPrefs\": data.params.get(\"implicitPrefs\", \"N/A\"),\n",
    "            \"rmse\": data.metrics.get(\"rmse\", None),\n",
    "            \"precision\": data.metrics.get(\"precision\", None),\n",
    "            \"recall\": data.metrics.get(\"recall\", None)\n",
    "        })\n",
    "'''\n",
    "\n",
    "        rows.append({\n",
    "            \"run_id\": run.info.run_id,\n",
    "            \"model\": data.params.get(\"model\"),             \n",
    "            \"rank\": data.params.get(\"rank\"),\n",
    "            \"regParam\": data.params.get(\"regParam\"),\n",
    "            \"maxIter\": data.params.get(\"maxIter\"),\n",
    "            \"implicitPrefs\": data.params.get(\"implicitPrefs\"),\n",
    "            \"rmse\": data.metrics.get(\"rmse\"),\n",
    "            \"precision\": data.metrics.get(\"Precision@10\"),\n",
    "            \"recall\": data.metrics.get(\"Recall@10\")\n",
    "        })\n",
    "\n",
    "\n",
    "    # Define the DataFrame schema\n",
    "    schema = StructType([\n",
    "        StructField(\"run_id\", StringType(), True),\n",
    "        StructField(\"model\", StringType(), True),\n",
    "        StructField(\"rank\", StringType(), True),\n",
    "        StructField(\"regParam\", StringType(), True),\n",
    "        StructField(\"maxIter\", StringType(), True),\n",
    "        StructField(\"implicitPrefs\", StringType(), True),\n",
    "        StructField(\"rmse\", FloatType(), True),\n",
    "        StructField(\"precision\", FloatType(), True),\n",
    "        StructField(\"recall\", FloatType(), True)\n",
    "    ])\n",
    "\n",
    "    # Convert rows to Spark DataFrame using the eschema\n",
    "    mlflow_log_df = spark.createDataFrame([Row(**row) for row in rows], schema)\n",
    "    # Display the DataFrame\n",
    "    mlflow_log_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd506bbc-44e9-407e-b25d-b687c3cc70fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Delete all MLflow logs\n",
    "\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Set experiment path \n",
    "experiment_path = \"/Users/j.l.wong@edu.salford.ac.uk/game_recommender_system\"\n",
    "client = MlflowClient()\n",
    "experiment = client.get_experiment_by_name(experiment_path)\n",
    "\n",
    "if experiment is not None:\n",
    "    experiment_id = experiment.experiment_id\n",
    "    runs = client.search_runs(experiment_id)\n",
    "    for run in runs:\n",
    "        print(f\"Deleting run: {run.info.run_id}\")\n",
    "        client.delete_run(run.info.run_id)\n",
    "    print(\"All runs deleted.\")\n",
    "else:\n",
    "    print(\" Experiment not found.\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1210934385966602,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "game_recommender_system",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}